{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# UNEP link localizer\n",
        "This online tool finds the translated webpage of an UN website.\n",
        "- **`Input`**: https://www.unep.org/news-and-stories/story/climate-crisis-alters-their-lands-indigenous-peoples-turn-courts\n",
        "\n",
        "- **`Output`**: https://www.unep.org/es/noticias-y-reportajes/reportajes/los-pueblos-indigenas-recurren-los-tribunales-ante-la-crisis\n",
        "\n",
        "It does it for all hyperlinks inside a Word document, a PDF or an UNEP story.\n",
        "\n",
        "*Created by [@NelsonJQ](https://www.linkedin.com/in/nelson-jaimesq/)*\n",
        "\n",
        "⬇️⬇️⬇️\n"
      ],
      "metadata": {
        "id": "gUKYFcrgXudY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title *Prepare* { display-mode: \"form\", run: \"auto\" }\n",
        "#Code created by Nelson JAIMES-QUINTERO\n",
        "#@markdown # **Prepare the code**\n",
        "#@markdown Execute to install necessary packages and modules\n",
        "\n",
        "#@markdown No need to mount on Google Drive.\n",
        "#mount_gdrive = True #@param {type:\"boolean\"}\n",
        "##@markdown Force mount again. Useful for bug cases:\n",
        "#force_remount = False #@param {type:\"boolean\"}\n",
        "# Create the language dictionary\n",
        "# TODO to improve many possibilities like Chinese: [\"ch\", \"cn\", \"zh\", \"zh-hans\"]\n",
        "\n",
        "!pip install beautifulsoup4\n",
        "!pip install pypandoc\n",
        "!pip install python-docx\n",
        "!pip install Spire.Doc\n",
        "!pip install pymupdf\n",
        "\n",
        "import re\n",
        "import requests\n",
        "import sys\n",
        "import os\n",
        "import urllib\n",
        "from bs4 import BeautifulSoup\n",
        "import urllib.parse\n",
        "from urllib.parse import urlparse, urljoin\n",
        "from urllib3.exceptions import InsecureRequestWarning\n",
        "from urllib3 import disable_warnings\n",
        "import email.utils\n",
        "import pandas as pd\n",
        "import pypandoc\n",
        "import fitz\n",
        "from docx import Document\n",
        "from spire.doc import *\n",
        "from spire.doc.common import *\n",
        "\n",
        "disable_warnings(InsecureRequestWarning)\n",
        "\n",
        "\n",
        "def get_language_code(query):\n",
        "    \"\"\"\n",
        "    Search for a value given a key or search for a key given a value in the language_dict.\n",
        "\n",
        "    Args:\n",
        "        query (str): The key or value to search for.\n",
        "\n",
        "    Returns:\n",
        "        str: The corresponding value or key.\n",
        "    \"\"\"\n",
        "    for key, value in language_dict.items():\n",
        "        if query.lower() == key.lower():\n",
        "            return value\n",
        "        elif query.lower() == value.lower():\n",
        "            return key\n",
        "\n",
        "    return None\n",
        "\n",
        "# Example usage:\n",
        "language_dict = {\n",
        "    \"Spanish\": \"es\",\n",
        "    \"French\": \"fr\",\n",
        "    \"Swahili\": \"sw\",\n",
        "    \"English\": \"en\",\n",
        "    \"Chinese\": \"zh-hans\",\n",
        "    \"Portuguese\": \"pt-br\",\n",
        "    \"Russian\": \"ru\",\n",
        "    \"Arabic\": \"ar\"\n",
        "}\n",
        "\n",
        "#result_key = get_language_code(\"Spanish\")\n",
        "#result_value = get_language_code(\"fr\")\n",
        "\n",
        "#print(result_key)    # Output: \"fr\"\n",
        "#print(result_value)  # Output: \"Spanish\"\n",
        "#print(type(result_value))\n",
        "\n",
        "# Extract node's number from UNEP URL\n",
        "def find_UNEP_node(unep_full_link: str) -> str:\n",
        "  \"\"\"find_UNEP_node access the input URL, finds the language version\n",
        "  of the webpage, return the URL's node that is common to all UNEP languages.\n",
        "\n",
        "  Args:\n",
        "    unep_full_link (str): String of full web url in UNEP website.\n",
        "\n",
        "  Returns:\n",
        "    str: URL's node\n",
        "\n",
        "  Examples:\n",
        "    >>> convert_UNEP_url('https://www.unep.org/news-and-stories/story/climate-crisis-alters-their-lands-indigenous-peoples-turn-courts')\n",
        "    '34817'\n",
        "  \"\"\"\n",
        "  # Send an URL request with headers to bypass CloudFlare as suggested by https://stackoverflow.com/a/74674276\n",
        "  req = urllib.request.Request(unep_full_link)\n",
        "  req.add_header('User-Agent', 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:106.0) Gecko/20100101 Firefox/106.0')\n",
        "  req.add_header('Accept', 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8')\n",
        "  req.add_header('Accept-Language', 'en-US,en;q=0.5')\n",
        "  try:\n",
        "      response = urllib.request.urlopen(req)\n",
        "\n",
        "  except urllib.error.HTTPError as e:\n",
        "      print(f\"HTTPError: {e.code} - {e.reason}\")\n",
        "      # You can raise a custom exception or handle the error in any other way\n",
        "  except urllib.error.URLError as e:\n",
        "      print(f\"URLError: {e.reason}\")\n",
        "      # Handle other URL-related errors\n",
        "  except Exception as e:\n",
        "      print(f\"An unexpected error occurred: {e}\")\n",
        "      # Handle other unexpected errors\n",
        "  else:\n",
        "      # If no exception occurred, continue text processing\n",
        "      print(\"Scraping successful\")\n",
        "\n",
        "      r = urllib.request.urlopen(req).read().decode('utf-8')\n",
        "      if r:\n",
        "        # Convert html into BeautifulSoup object\n",
        "        soup = BeautifulSoup(r, 'html.parser')\n",
        "        #print(soup)\n",
        "\n",
        "        # Find the <ul> element with class 'links'\n",
        "        ul_element = soup.find('ul', class_='links')\n",
        "\n",
        "        # Find the <li> element with class 'es is-active'\n",
        "        li_element = ul_element.find('li', class_=lambda x: x and x.endswith('is-active'))\n",
        "\n",
        "        # Extract the value of the 'data-drupal-link-system-path' attribute\n",
        "        attribute_value = li_element.get('data-drupal-link-system-path')\n",
        "        return attribute_value.split('node/')[1]\n",
        "\n",
        "# test\n",
        "#print(find_UNEP_node('https://www.unep.org/news-and-stories/story/climate-crisis-alters-their-lands-indigenous-peoples-turn-courts'))\n",
        "#print(type(find_UNEP_node('https://www.unep.org/news-and-stories/story/climate-crisis-alters-their-lands-indigenous-peoples-turn-courts')))\n",
        "\n",
        "# Main function: finds the language version of a web article in UNEP website.\n",
        "\n",
        "def convert_UNEP_url(unep_full_link: str, target_lang: str = 'en') -> str:\n",
        "  \"\"\"convert_UNEP_url access the input URL, finds the URL of the translated version\n",
        "  of the webpage in the input language, return an URL.\n",
        "\n",
        "  Args:\n",
        "    unep_full_link (str): String of full web url in UNEP website.\n",
        "    target_lang (str): Target language, default = 'en'.\n",
        "\n",
        "  Returns:\n",
        "    str: New converted URL\n",
        "\n",
        "  Examples:\n",
        "    >>> convert_UNEP_url('https://www.unep.org/news-and-stories/story/climate-crisis-alters-their-lands-indigenous-peoples-turn-courts', 'es')\n",
        "    'https://www.unep.org/es/noticias-y-reportajes/reportajes/los-pueblos-indigenas-recurren-los-tribunales-ante-la-crisis'\n",
        "  \"\"\"\n",
        "  # Send an URL request with headers to bypass CloudFlare as suggested by https://stackoverflow.com/a/74674276\n",
        "  req = urllib.request.Request(unep_full_link)\n",
        "  req.add_header('User-Agent', 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:106.0) Gecko/20100101 Firefox/106.0')\n",
        "  req.add_header('Accept', 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8')\n",
        "  req.add_header('Accept-Language', 'en-US,en;q=0.5')\n",
        "  try:\n",
        "      response = urllib.request.urlopen(req)\n",
        "\n",
        "  except urllib.error.HTTPError as e:\n",
        "      print(f\"HTTPError: {e.code} - {e.reason}\")\n",
        "      # You can raise a custom exception or handle the error in any other way\n",
        "      return None\n",
        "  except urllib.error.URLError as e:\n",
        "      print(f\"URLError: {e.reason}\")\n",
        "      # Handle other URL-related errors\n",
        "      return None\n",
        "  except Exception as e:\n",
        "      print(f\"An unexpected error occurred: {e}\")\n",
        "      # Handle other unexpected errors\n",
        "      return None\n",
        "  else:\n",
        "      # If no exception occurred, continue text processing\n",
        "      print(\"Scraping successful\")\n",
        "\n",
        "      r = urllib.request.urlopen(req).read().decode('utf-8')\n",
        "      if r:\n",
        "        # Convert html into BeautifulSoup object\n",
        "        soup = BeautifulSoup(r, 'html.parser')\n",
        "        #print(soup)\n",
        "\n",
        "        # Looks for the link in the target language, whose class is \"language-link\"\n",
        "        lenguas = soup.find(\"a\", class_=\"language-link\", hreflang = target_lang)\n",
        "        #print(lenguas)\n",
        "        if lenguas:\n",
        "          #print(f\"https://www.unep.org{lenguas['href']}\")\n",
        "          if lenguas['href'].endswith('/node'):\n",
        "            return f\"https://www.unep.org{lenguas['href'][0:-5]}\"\n",
        "          return f\"https://www.unep.org{lenguas['href']}\"\n",
        "        elif not lenguas:\n",
        "            # Find the <ul> element with class 'links'\n",
        "          ul_element = soup.find('ul', class_='links')\n",
        "          if ul_element:\n",
        "          # Find the <li> element with class 'es is-active'\n",
        "            li_element = ul_element.find('li', class_=lambda x: x and x.endswith('is-active'))\n",
        "\n",
        "          # Extract the value of the 'data-drupal-link-system-path' attribute\n",
        "            node_value = li_element.get('data-drupal-link-system-path')\n",
        "            return find_from_nodeLink(int(node_value.split(\"/\")[1]), target_lang)\n",
        "            #return f\"https://www.unep.org/{node_value}\"\n",
        "        else:\n",
        "          raise ValueError(\"Error: Webpage accessed but the tag 'a', class_='language-link' was not found. Probably because the website was blocked by firewall/CloudFlare\")\n",
        "          return None\n",
        "      else:\n",
        "        print(\"\\n<-- Error code. The programme could not access the webpage, forbidden\")\n",
        "        return None\n",
        "\n",
        "# test\n",
        "#input = input(\"Enter your UNEP url:\")\n",
        "#input = 'https://www.unep.org/news-and-stories/story/climate-crisis-alters-their-lands-indigenous-peoples-turn-courts'\n",
        "#input = \"https://www.unep.org/ru\"\n",
        "#print(convert_UNEP_url(input, 'es'))\n",
        "#print(convert_UNEP_url(input, 'fr'))\n",
        "\n",
        "\n",
        "UNEP_LANG_CODES = ['ar', 'es', 'fr', 'ru', 'sw', 'pt-br', 'ch', 'zh', 'zh-hans', 'en']\n",
        "\n",
        "def find_from_nodeLink(node_input, target_lang='empty'):\n",
        "    \"\"\"Replaces a node_link to the corresponding language.\n",
        "\n",
        "    Args:\n",
        "        node_input (str, int): Either a string of web URL containing the word 'node' and its ID, or an integer ID (or a string representation of an integer).\n",
        "        target_lang (str): Target language, default = 'empty'.\n",
        "\n",
        "    Returns:\n",
        "        str: New converted URL\n",
        "\n",
        "    Examples:\n",
        "        >>> find_from_nodeLink('https://www.unep.org/pt-br/node/30010', 'fr')\n",
        "        'https://www.unep.org/fr/node/30010'\n",
        "        >>> find_from_nodeLink('https://www.unep.org/pt-br/node/30010', 'empty')\n",
        "        'https://www.unep.org/node/30010'\n",
        "        >>> find_from_nodeLink('https://www.unep.org/pt-br/node/30010', 'zh-hans')\n",
        "        'https://www.unep.org/zh-hans/node/30010'\n",
        "        >>> find_from_nodeLink(30010, 'fr')\n",
        "        'https://www.unep.org/fr/node/30010'\n",
        "        >>> find_from_nodeLink('30010', 'fr')\n",
        "        'https://www.unep.org/fr/node/30010'\n",
        "    \"\"\"\n",
        "\n",
        "    if isinstance(node_input, str) and node_input.isdigit():\n",
        "        node_input = int(node_input)\n",
        "\n",
        "    if isinstance(node_input, int):\n",
        "        node_url = f'https://www.unep.org/{target_lang}/node/{node_input}'\n",
        "    elif isinstance(node_input, str):\n",
        "        node_url = node_input\n",
        "    else:\n",
        "        raise ValueError(\"Error: Provide either a string URL or an integer ID (or a string representation of an integer)\")\n",
        "\n",
        "    pattern = r\"https://www\\.unep\\.org/[a-z]*-?[a-z]*/?node/(\\d+)\"\n",
        "\n",
        "    if target_lang == \"empty\":\n",
        "        target_lang = \"en\"\n",
        "    if target_lang in [\"ch\", 'zh', 'cn']:\n",
        "      target_lang = \"zh-hans\"\n",
        "    if target_lang in ['pt', 'pt-pt']:\n",
        "      target_lang = \"pt-br\"\n",
        "    if target_lang in UNEP_LANG_CODES:\n",
        "        if re.findall(pattern, node_url):\n",
        "            # Replace the language part in the URL\n",
        "            new_url = re.sub(pattern, r\"https://www.unep.org/{}/node/\\1\".format(target_lang), node_url)\n",
        "            return new_url\n",
        "        else:\n",
        "            raise ValueError(\"Error: URL not found, or website blocked by firewall/CloudFare\")\n",
        "    else:\n",
        "        raise ValueError(\"Error: Provide a language code among these: 'ar','es','fr','ru','sw','pt-br','zh-hans', 'en' or leave empty\")\n",
        "\n",
        " # Generic scraper\n",
        "\n",
        "def get_HTML_generic(any_url: str) -> BeautifulSoup:\n",
        "\n",
        "  \"\"\"Any website link converter, it access the website and returns the HTML.\n",
        "\n",
        "  Args:\n",
        "  any_url (str): String of web url from the web wedocs.unep.org\n",
        "\n",
        "  Returns:\n",
        "    str: parsed HTML with BeautifulSoup\n",
        "\n",
        "  Example:\n",
        "    >>> convert_WeDocs_href('https://wedocs.unep.org/handle/20.500.11822/43104', 'Chinese')\n",
        "    'https://wedocs.unep.org/bitstream/handle/20.500.11822/43104/PracticalGuide_ZH.pdf?sequence=5&isAllowed=y'\n",
        "  \"\"\"\n",
        "  req = urllib.request.Request(any_url)\n",
        "  req.add_header('User-Agent', 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:106.0) Gecko/20100101 Firefox/106.0')\n",
        "  req.add_header('Accept', 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8')\n",
        "  req.add_header('Accept-Language', 'en-US,en;q=0.5')\n",
        "  try:\n",
        "      response = urllib.request.urlopen(req)\n",
        "\n",
        "  except urllib.error.HTTPError as e:\n",
        "      print(f\"HTTPError: {e.code} - {e.reason} when accessing {any_url}\")\n",
        "      # You can raise a custom exception or handle the error in any other way\n",
        "  except urllib.error.URLError as e:\n",
        "      print(f\"URLError: {e.reason} when accessing {any_url}\")\n",
        "      # Handle other URL-related errors\n",
        "  except Exception as e:\n",
        "      print(f\"An unexpected error occurred: {e} when accessing {any_url}\")\n",
        "      # Handle other unexpected errors\n",
        "  else:\n",
        "      # If no exception occurred, continue text processing\n",
        "      print(\"Scraping successful\")\n",
        "\n",
        "      r = urllib.request.urlopen(req).read().decode('utf-8')\n",
        "      if r:\n",
        "        # Convert html into BeautifulSoup object\n",
        "        soup = BeautifulSoup(r, 'html.parser')\n",
        "        return soup\n",
        "        #print(soup)\n",
        "\n",
        "# Example usage with an integer ID provided as a string\n",
        "#print(find_from_nodeLink('30010', 'fr'))\n",
        "#print(find_from_nodeLink(30010, 'fr'))\n",
        "#print(find_from_nodeLink('https://www.unep.org/pt-br/node/30010', 'zh-hans'))\n",
        "\n",
        "\n",
        "def try_lang_switcher(switcher_soup, lang_code: str, base_url) -> str:\n",
        "\n",
        "  # Find the <ul> element with class \"language-switcher\"\n",
        "  #language_switcher_ul = switcher_soup.find('ul', class_='language-switcher')\n",
        "  language_switcher_ul = switcher_soup.find('ul', class_=lambda value: value and value.startswith('language-switcher'))\n",
        "\n",
        "  # Extract href values from <a> elements within the <ul>\n",
        "  if language_switcher_ul:\n",
        "    href_values = [a['href'] for a in language_switcher_ul.find_all('a')]\n",
        "\n",
        "    for i, element in enumerate(href_values):\n",
        "          if lang_code in element:\n",
        "              new_link = urljoin(base_url, href_values[i])\n",
        "              return new_link\n",
        "  return None\n",
        "\n",
        "# Function to concatenate absolute paths if URL cannot be accessed\n",
        "from urllib.parse import urljoin\n",
        "\n",
        "def concatenate_missing_segments(arg1, arg2):\n",
        "    \"\"\"\n",
        "    Concatenates missing URL segments from Arg1 to Arg2.\n",
        "\n",
        "    Args:\n",
        "        arg1 (str): The URL containing the missing segments, longer URL like\n",
        "        \"https://www.unep.org/interactive/explore-ecosystems/mountains/en/index.php#/mountain-intro\"\n",
        "\n",
        "        arg2 (str): The target URL, shorter URL like\n",
        "        \"https://www.unep.org/interactive/explore-ecosystems/mountains/ar\"\n",
        "\n",
        "    Returns:\n",
        "        str: The concatenated URL.\n",
        "        \"https://www.unep.org/interactive/explore-ecosystems/mountains/ar/index.php#/mountain-intro\"\n",
        "    \"\"\"\n",
        "    if len(arg1)>len(arg2):\n",
        "      missing_segment = arg1[len(arg2):]\n",
        "      return arg2 + missing_segment\n",
        "\n",
        "# Example usage:\n",
        "#arg1 = \"https://www.unep.org/interactive/explore-ecosystems/mountains/en/index.php#/mountain-intro\"\n",
        "#arg2 = \"https://www.unep.org/interactive/explore-ecosystems/mountains/ar\"\n",
        "\n",
        "#result = concatenate_missing_segments(arg1, arg2)\n",
        "#print(result)\n",
        "\n",
        "\n",
        "def convert_URL_anyWebsite(any_web_url: str, lang_code) -> str:\n",
        "  # Access the URL to get the HTML with BeautifulSoup --> soup object\n",
        "  sauce_html = get_HTML_generic(any_web_url)\n",
        "  print(type(sauce_html))\n",
        "  if sauce_html:\n",
        "    # Search the language_switcher HTML tag and gets the language code\n",
        "    switcher_link = try_lang_switcher(sauce_html, lang_code.lower(), any_web_url)\n",
        "    if switcher_link and get_HTML_generic(switcher_link):\n",
        "      return switcher_link\n",
        "    elif switcher_link:\n",
        "      return concatenate_missing_segments(any_web_url, switcher_link)\n",
        "    elif sauce_html.find_all(lambda tag: tag.has_attr('data-sf-role') and tag['data-sf-role'] == lang_code):    #working for WHO news\n",
        "      print(\"trying WHO\")\n",
        "      matching_tags = sauce_html.find_all(lambda tag: tag.has_attr('data-sf-role') and tag['data-sf-role'] == lang_code)\n",
        "      if matching_tags:\n",
        "        print(matching_tags)\n",
        "        return matching_tags[0]['value']\n",
        "    elif sauce_html.find_all(lambda tag: tag.has_attr('hreflang') and tag['hreflang'] == lang_code):\n",
        "      print(\"trying hreflang\")\n",
        "      matching_tags = sauce_html.find_all(lambda tag: tag.has_attr('hreflang') and tag['hreflang'] == lang_code)\n",
        "      if matching_tags:\n",
        "        return matching_tags[0]['href']\n",
        "    elif sauce_html:\n",
        "      print(\"trying language_link\") # working for UNESCO\n",
        "      lang_tag = sauce_html.find(\"a\", class_=\"language-link\", hreflang = lang_code)\n",
        "      #print(lang_tag)\n",
        "      if lang_tag != None:\n",
        "        return urljoin(any_web_url, lang_tag['href'])\n",
        "    else:\n",
        "      return None\n",
        "#output_li = convert_URL_anyWebsite(\"cleanairblueskies@un.org\", \"es\")\n",
        "#print(output_li)\n",
        "\n",
        "def weDocs_short(weDocs_url) -> str:\n",
        "  \"\"\"Replaces a language specific WeDocs link with the landing page\n",
        "\n",
        "  Args:\n",
        "    weDocs_url (str): String of web url from the web wedocs.unep.org\n",
        "\n",
        "  Returns:\n",
        "    str: Landing page of the document, so it is not language specific.\n",
        "\n",
        "  Example:\n",
        "    >>> weDocs_short('https://wedocs.unep.org/bitstream/handle/20.500.11822/43104/Practical_Guide.pdf?sequence=1&isAllowed=y')\n",
        "    'https://wedocs.unep.org/handle/20.500.11822/43104/'\n",
        "  \"\"\"\n",
        "  return re.sub(r\"https://wedocs.unep.org/(bitstream/)?handle/([\\w.-]+/\\d+).+\", r\"https://wedocs.unep.org/handle/\\2\", weDocs_url)\n",
        "\n",
        "# WeDocs link converter, it access a short WeDocs link and returns a language-specific URL (pdf)\n",
        "\n",
        "\n",
        "def convert_WeDocs_href(url: str, target_lang: str ='English') -> str:\n",
        "\n",
        "    \"\"\"WeDocs link converter, it access a short WeDocs link\n",
        "    and returns a language-specific URL (pdf)\n",
        "\n",
        "    Args:\n",
        "    weDocs_url (str): String of web url from the web wedocs.unep.org\n",
        "    target_lang (str): Language code of the document to find.\n",
        "\n",
        "    Returns:\n",
        "      str: Download link of the PDF in the language requested.\n",
        "\n",
        "    Example:\n",
        "      >>> convert_WeDocs_href('https://wedocs.unep.org/handle/20.500.11822/43104', 'Chinese')\n",
        "      'https://wedocs.unep.org/bitstream/handle/20.500.11822/43104/PracticalGuide_ZH.pdf?sequence=5&isAllowed=y'\n",
        "\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Send an HTTP GET request to the URL\n",
        "        response = requests.get(url, verify=False)\n",
        "\n",
        "        # Check if the request was successful\n",
        "        if response.status_code == 200:\n",
        "            # Parse the HTML content using BeautifulSoup\n",
        "            pattern = re.compile(r\".*{}.*\".format(re.escape(target_lang.capitalize())))    # TODO normalize to take into account the Dico's key, in case user enters RU instead of Russian\n",
        "            soup = BeautifulSoup(response.text, 'html.parser')\n",
        "            #print(soup.prettify())\n",
        "\n",
        "            # Find the <a> tag with the word \"Spanish\" or the entered language name in its text\n",
        "            # Extract the href attribute value\n",
        "            lang_link = soup.find(string=re.compile(pattern)).parent['href']\n",
        "            #print(lang_link)\n",
        "\n",
        "            if lang_link:\n",
        "                # Merge the domain and PDF name to create the complete link\n",
        "              clean_link = \"https://wedocs.unep.org\" + lang_link\n",
        "              return clean_link\n",
        "            else:\n",
        "                return f\"No link with '{target_lang}' text found.\"\n",
        "        else:\n",
        "            return \"Failed to retrieve the URL.\"\n",
        "\n",
        "    except Exception as e:\n",
        "        return str(e)\n",
        "\n",
        "#spanish_href = extract_WeDocs_href(url, \"Spanish\")\n",
        "#portuguese_href = extract_WeDocs_href(url, \"Portuguese\")\n",
        "#ch_href = convert_WeDocs_href(url, \"Chinese\")\n",
        "#print(spanish_href)\n",
        "#print(portuguese_href)\n",
        "#print(ch_href)\n",
        "\n",
        "def access_un_library_by_id(user_input_id):\n",
        "    try:\n",
        "        # Base URL\n",
        "        base_url = \"https://digitallibrary.un.org/search?\"\n",
        "\n",
        "        # Construct the URL with the user-provided ID\n",
        "        url = f\"{base_url}ln=fr&p={user_input_id}&f=&c=Resource%20Type&c=UN%20Bodies&sf=&so=d&rg=50&fti=0\"\n",
        "\n",
        "        # Send an HTTP GET request to the URL\n",
        "        response = requests.get(url)\n",
        "\n",
        "        # Check if the request was successful (status code 200)\n",
        "        if response.status_code == 200:\n",
        "            print(\"Request was successful. Content:\")\n",
        "\n",
        "            # Parse the HTML content using BeautifulSoup\n",
        "            soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "            # Find the <div> with class=\"result-title\"\n",
        "            result_title_div = soup.find('div', class_='result-title')\n",
        "\n",
        "            if result_title_div:\n",
        "                # Find the first <a> tag within the result-title div and get its href value\n",
        "                result_title_a = result_title_div.find('a', href=True)\n",
        "                if result_title_a:\n",
        "                    href_value = result_title_a['href']\n",
        "                    return f\"https://digitallibrary.un.org{href_value}\"\n",
        "                else:\n",
        "                    print(\"No <a> tag found inside result-title.\")\n",
        "            else:\n",
        "                print(\"No result-title div found in the HTML.\")\n",
        "                return None\n",
        "\n",
        "\n",
        "        else:\n",
        "            print(f\"Failed to retrieve the URL. Status code: {response.status_code}\")\n",
        "            return None\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "# Get user input for the ID\n",
        "#user_input_id = input(\"Enter the ID: \")\n",
        "\n",
        "# Call the function with user input\n",
        "#resultado = access_un_library_by_id(user_input_id)\n",
        "#print(resultado)\n",
        "\n",
        "  # Send an URL request with headers to bypass CloudFlare as suggested by https://stackoverflow.com/a/74674276\n",
        "def access_un_library_byResourceURL(landing_url: str) -> BeautifulSoup:\n",
        "    req = urllib.request.Request(landing_url)\n",
        "    req.add_header('User-Agent', 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:106.0) Gecko/20100101 Firefox/106.0')\n",
        "    req.add_header('Accept', 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8')\n",
        "    req.add_header('Accept-Language', 'en-US,en;q=0.5')\n",
        "    try:\n",
        "        response = urllib.request.urlopen(req)\n",
        "\n",
        "    except urllib.error.HTTPError as e:\n",
        "        print(f\"HTTPError: {e.code} - {e.reason}\")\n",
        "        # You can raise a custom exception or handle the error in any other way\n",
        "        return None\n",
        "    except urllib.error.URLError as e:\n",
        "        print(f\"URLError: {e.reason}\")\n",
        "        return None\n",
        "        # Handle other URL-related errors\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred: {e}\")\n",
        "        return None\n",
        "        # Handle other unexpected errors\n",
        "    else:\n",
        "        # If no exception occurred, continue text processing\n",
        "        print(\"Scraping successful\")\n",
        "\n",
        "        r = urllib.request.urlopen(req).read().decode('utf-8')\n",
        "        if r:\n",
        "          # Convert html into BeautifulSoup object\n",
        "          soup = BeautifulSoup(r, 'html.parser')\n",
        "          #print(soup)\n",
        "          return soup\n",
        "        else:\n",
        "          # HTML error\n",
        "          raise ValueError(\"Error in parsing the website content in HTML\")\n",
        "          return None\n",
        "\n",
        "\n",
        "\n",
        "def extract_info_UNdocLink(url, lang2_code):\n",
        "    \"\"\"\n",
        "    Extracts information from a given UNDocs URL.\n",
        "\n",
        "    Args:\n",
        "        url (str): The UNDocs URL.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing the extracted information.\n",
        "    \"\"\"\n",
        "    # Define a regex pattern to match the components in the URL\n",
        "    # https://undocs.org/en/UNEP/EA.5/28/Corr.1\n",
        "    pattern = r'https://undocs\\.org/([a-z]{2})?/?([A-Z]+)/(.*?)/(\\d+)/(.*?)$'\n",
        "\n",
        "    # Use regex to find the components in the URL\n",
        "    match = re.match(pattern, url)\n",
        "\n",
        "    if match:\n",
        "        record_id = match.group(0)\n",
        "        symbol = match.group(2)\n",
        "        doc_type = match.group(3)\n",
        "        unga = match.group(4)\n",
        "        resolution_id = match.group(5)\n",
        "        language_code = match.group(1) if match.group(1) else None  # Optional language code\n",
        "        return  f\"https://undocs.org/{lang2_code.lower()}/{symbol}/{doc_type}/{unga}/{resolution_id}\"\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "# Example usage:\n",
        "#url = \"https://undocs.org/en/UNEP/EA.5/28/Corr.1\"\n",
        "#result = extract_info_UNdocLink(url, \"fr\")\n",
        "#print(result)\n",
        "\n",
        "# Define the language dictionary\n",
        "language_dict = {\n",
        "    \"Spanish\": \"es\",\n",
        "    \"French\": \"fr\",\n",
        "    \"English\": \"en\",\n",
        "    \"Chinese\": \"ch\",\n",
        "    \"Russian\": \"ru\",\n",
        "    \"Arabic\": \"ar\"\n",
        "}\n",
        "\n",
        "#input_language = \"Russian\"\n",
        "\n",
        "# 1.7 UN Docs\n",
        "def get_jobID_undocs(url):\n",
        "    \"\"\"\n",
        "    Extracts the job ID from a given URL of the ny.un.org website.\n",
        "\n",
        "    Args:\n",
        "        url (str): The URL of the document on ny.un.org.\n",
        "\n",
        "    Returns:\n",
        "        str: The extracted job ID.\n",
        "    \"\"\"\n",
        "    # Define a regex pattern to match the job ID in the URL\n",
        "    pattern = r'dds-ny.*/([A-Za-z0-9]+)\\.pdf'\n",
        "\n",
        "    # Use regex to find the job ID in the URL\n",
        "    match = re.search(pattern, url)\n",
        "\n",
        "    # Return the matched job ID or None if not found\n",
        "    return match.group(1) if match else None\n",
        "\n",
        "\n",
        "\n",
        "# Extract the `value` attribute of <option> tags with the specified regex pattern\n",
        "def find_lang_UNdoc(un_docs_link, input_language):\n",
        "    un_library_url = un_docs_link\n",
        "\n",
        "    # Define the language dictionary\n",
        "    UN_languages_dict = {\n",
        "        \"Spanish\": \"es\",\n",
        "        \"French\": \"fr\",\n",
        "        \"English\": \"en\",\n",
        "        \"Chinese\": \"ch\",\n",
        "        \"Russian\": \"ru\",\n",
        "        \"Arabic\": \"ar\"\n",
        "    }\n",
        "\n",
        "    if \"undocs.org\" in un_docs_link:\n",
        "      #return extract_info_UNdocLink(un_docs_link, UN_languages_dict[input_language])\n",
        "      return extract_info_UNdocLink(un_docs_link, input_language)\n",
        "    elif \"dds-ny\" in un_docs_link:\n",
        "      #extract ID TODO\n",
        "      un_library_url_ID = get_jobID_undocs(un_docs_link)\n",
        "      print(un_library_url_ID)\n",
        "      # Get URL from ID\n",
        "      un_library_url = access_un_library_by_id(un_library_url_ID)\n",
        "      print(un_library_url)\n",
        "\n",
        "    elif \"digitallibrary.un.org\" in un_docs_link:\n",
        "      un_library_url = un_docs_link\n",
        "\n",
        "\n",
        "    try:\n",
        "    # Get HTML from UN_lib URL\n",
        "    #soup = access_un_library_byResourceURL(un_library_url)\n",
        "      my_soup = access_un_library_byResourceURL(un_library_url)\n",
        "      if my_soup is None:\n",
        "        return None\n",
        "    except Exception as e:\n",
        "      print(f\"An unexpected error occurred: {e}\")\n",
        "      return None\n",
        "    else:\n",
        "      # Define the regex pattern\n",
        "      regex_pattern = r\"-(\\w{2})\\.pdf\"\n",
        "\n",
        "      # Find all <option> tags\n",
        "      options = my_soup.find_all('option', value=re.compile(regex_pattern))\n",
        "\n",
        "      # Extract and print the `value` attribute values\n",
        "      for option in options:\n",
        "          value = option['value']\n",
        "          match = re.search(regex_pattern, value)\n",
        "          if match:\n",
        "              language_code = match.group(1)\n",
        "              # Check if the language code is in the language_dict\n",
        "              language = next((k for k, v in UN_languages_dict.items() if v.startswith(language_code.lower())), 'Unknown')\n",
        "\n",
        "              #print(f\"Option Value: {value}, Language Code: {language_code}, Language: {language}\")\n",
        "\n",
        "              # Prepare the direct link for the requested language\n",
        "              if language == input_language:\n",
        "                output_links = [value]\n",
        "\n",
        "                # Define a regular expression pattern with capture groups\n",
        "                pattern = r\"https://digitallibrary.un.org/record/(\\d+)/files/([A-Z]+)_([A-Z]+)_([\\d]+)_([\\d]+)-(\\w{2})\\.pdf\"\n",
        "\n",
        "                # Use re.search to find matches and capture groups\n",
        "                match = re.search(pattern, value)\n",
        "\n",
        "                if match:\n",
        "                    # Extract capture group values\n",
        "                    record_id = match.group(1)\n",
        "                    symbol = match.group(2)         # A\n",
        "                    doc_type = match.group(3)       # RES\n",
        "                    unga = match.group(4)           # 61\n",
        "                    resolution_id = match.group(5)  # 295\n",
        "                    language_code = match.group(6)  # es\n",
        "\n",
        "                    # Construct the output string  # https://undocs.org/es/A/RES/61/295\n",
        "                    output_links.append(f\"https://undocs.org/{symbol}/{doc_type}/{unga}/{resolution_id}\")\n",
        "                    output_links.append(f\"https://undocs.org/{language_code.lower()}/{symbol}/{doc_type}/{unga}/{resolution_id}\")\n",
        "\n",
        "                else:\n",
        "                    print(\"No match found for the input string.\")\n",
        "                # Output is a list of 3 links:\n",
        "                  # 1 is UN Library:  https://digitallibrary.un.org/record/606782/files/A_RES_61_295-ZH.pdf\n",
        "                  # 2 is UN Docs multilingual shortlink: https://undocs.org/A/RES/61/295\n",
        "                  # 3 is UN Docs MONO-lingual shortlink: https://undocs.org/zh/A/RES/61/295\n",
        "\n",
        "                return output_links\n",
        "\n",
        "\n",
        "# Call the function to extract and print the option values\n",
        "#print(find_lang_UNdoc(\"https://undocs.org/en/UNEP/EA.5/28/Corr.1\", \"Russian\"))\n",
        "#print(get_language_code(\"fr\"))\n",
        "#print(find_lang_UNdoc(\"https://www.ohchr.org/en/documents/thematic-reports/ahrc3917-report-special-rapporteur-rights-indigenous-peoples\", get_language_code(\"fr\")))\n",
        "\n",
        "import re\n",
        "def convert_Intl_Day(url, language_code):\n",
        "    \"\"\"\n",
        "    Converts the language code in a UN URL to the specified language.\n",
        "\n",
        "    Args:\n",
        "        url (str): The UN URL.\n",
        "        language_code (str): The target language code.\n",
        "\n",
        "    Returns:\n",
        "        str: The modified URL with the specified language code.\n",
        "    \"\"\"\n",
        "\n",
        "    # Use regex to replace the language code in the URL\n",
        "    if language_code.lower() == \"ch\":\n",
        "      return re.sub(r'/([a-z]{2})/observances', f'/zh/observances', url)\n",
        "    else:\n",
        "      return re.sub(r'/([a-z]{2})/observances', f'/{language_code}/observances', url)\n",
        "\n",
        "\n",
        "# Example usage:\n",
        "#url = \"https://www.un.org/es/observances/cities-day\"\n",
        "#modified_url = convert_Intl_Day(url, \"ch\")\n",
        "#print(modified_url)\n",
        "\n",
        "import re\n",
        "\n",
        "def convert_URLendingBy_langEqualsCode(url, language_code):\n",
        "    \"\"\"\n",
        "    Converts the language code in a URL with the pattern ?lang=[A-Z]{2} to the specified language.\n",
        "    No URL validation.\n",
        "\n",
        "    Args:\n",
        "        url (str): The URL.\n",
        "        language_code (str): The target language code.\n",
        "\n",
        "    Returns:\n",
        "        str: The modified URL with the specified language code.\n",
        "    \"\"\"\n",
        "\n",
        "    if language_code.lower() == \"ch\":\n",
        "      return re.sub(r'(\\?lang=)[A-Z]{2}', fr'\\1ZH', url)\n",
        "    else:\n",
        "    # Use regex to replace the language code in the URL\n",
        "      return re.sub(r'(\\?lang=)[A-Z]{2}', fr'\\1{language_code.upper()}', url)\n",
        "\n",
        "\n",
        "# Example usage:\n",
        "#url = \"https://www.unep.org/interactives/beat-plastic-pollution/?lang=ES\"\n",
        "#modified_url = convert_URLendingBy_langEqualsCode(url, \"ch\")\n",
        "#print(modified_url)\n",
        "\n",
        "# Ultimate finder function\n",
        "\n",
        "def localize_URL(mi_URL: str, lengua: str=\"en\") -> str:\n",
        "\n",
        "  '''Apply all functions to try to find a language version of the input webpage\n",
        "  in the provided language code.\n",
        "\n",
        "  '''\n",
        "  resulting_link = None\n",
        "\n",
        "\n",
        "\n",
        "  def is_email(string):\n",
        "      print(f\"Validating if {string} is an email:\")\n",
        "      email_pattern = re.compile(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b')\n",
        "      return bool(email_pattern.match(string))\n",
        "\n",
        "  # Check if URL is not an email\n",
        "\n",
        "  if is_email(mi_URL):\n",
        "      print(f\"{mi_URL} is an email\")\n",
        "      return None\n",
        "  else:\n",
        "\n",
        "    #try UN Docs\n",
        "    #TODO find a way to scrape this search engine https://documents.un.org/prod/ods.nsf/home.xsp\n",
        "    # or how to download the PDF, access the symbol tag and join the url to undocs.org/\n",
        "\n",
        "    print(\"Trying find_lang_UNdoc for \", mi_URL)\n",
        "    resulting_link = find_lang_UNdoc(mi_URL, get_language_code(lengua))\n",
        "    if resulting_link:\n",
        "      return resulting_link[-1]\n",
        "\n",
        "    # International Days\n",
        "    if \"/observances/\" in mi_URL and \"un.org/\" in mi_URL:\n",
        "      print(\"Trying convert_Intl_Day\")\n",
        "      resulting_link = convert_Intl_Day(mi_URL, lengua)\n",
        "      return resulting_link\n",
        "\n",
        "    #  WeDocs UNEP\n",
        "    if \"wedocs.unep.org\" in mi_URL:\n",
        "      print(\"Trying convert_WeDocs_href\")\n",
        "      short_weDocs_url = weDocs_short(mi_URL)\n",
        "      resulting_link = convert_WeDocs_href(short_weDocs_url, get_language_code(lengua))\n",
        "      return resulting_link\n",
        "\n",
        "    # try UNEP articles\n",
        "    if \"unep.org\" in mi_URL and \"wedocs\" not in mi_URL:\n",
        "      print(\"Trying convert_UNEP_url\")\n",
        "      resulting_link = convert_UNEP_url(mi_URL, lengua)\n",
        "      return resulting_link\n",
        "\n",
        "    elif \".pdf\" not in mi_URL:\n",
        "      print(\"Trying convert_URL_anyWebsite\")\n",
        "      resulting_link = convert_URL_anyWebsite(mi_URL, lengua)\n",
        "      print(resulting_link)\n",
        "      if resulting_link is not None:\n",
        "        return resulting_link\n",
        "    else:\n",
        "      return None\n",
        "\n",
        "\n",
        "#print(localize_URL(\"https://documents-dds-ny.un.org/doc/UNDOC/GEN/N06/512/07/PDF/N0651207.pdf?OpenElement\", \"fr\"))\n",
        "#print(localize_URL(\"https://documents-dds-ny.un.org/doc/UNDOC/GEN/G16/015/38/PDF/G1601538.pdf?OpenElement\", \"fr\"))\n",
        "#print(localize_URL(\"https://undocs.org/FCCC/CP/2015/10/Add.1\", \"fr\"))\n",
        "#print(localize_URL(\"https://www.un.org/en/observances/environment-in-war-protection-day\", \"fr\"))\n",
        "#print(localize_URL(url5, \"fr\"))\n",
        "\n",
        "\n",
        "\n",
        "def convert_docx_to_html(docx_file_path):\n",
        "    output = pypandoc.convert_file(docx_file_path, 'html')\n",
        "    return output\n",
        "\n",
        "def extract_href_attributes(html_content):\n",
        "    soup = BeautifulSoup(html_content, 'html.parser')\n",
        "    # creates a list\n",
        "    href_values = [a['href'] for a in soup.find_all('a', href=True)]\n",
        "    return href_values\n",
        "\n",
        "def generate_table_URLs_from_Docx(docx_path, lang_code):\n",
        "  # Open the document\n",
        "  document = Document(docx_path)\n",
        "\n",
        "  # Extract hyperlinks\n",
        "  input_urls = []\n",
        "  for paragraph in document.paragraphs:\n",
        "      for run in paragraph.runs:\n",
        "          hyperlink = run.hyperlink\n",
        "          if hyperlink is not None:\n",
        "            input_urls.append(hyperlink.address)\n",
        "\n",
        "  #input_urls\n",
        "  data = []\n",
        "\n",
        "\n",
        "  # Initialize lists to store data for the DataFrame\n",
        "  index_list = []\n",
        "  original_url_list = []\n",
        "  localized_url_list = []\n",
        "\n",
        "  # Apply localizeURL to each URL in the list\n",
        "  for index, url in enumerate(input_urls):\n",
        "      localized_url = localize_URL(url, lang_code)  # Replace 'en' with the desired language code\n",
        "      index_list.append(index)\n",
        "      original_url_list.append(url)\n",
        "      localized_url_list.append(localized_url)\n",
        "\n",
        "  # Create a DataFrame\n",
        "  df_docx = pd.DataFrame({\n",
        "      'index': index_list,\n",
        "      'url': original_url_list,\n",
        "      'localized_url': localized_url_list\n",
        "  })\n",
        "\n",
        "  # Export the DataFrame to a CSV file\n",
        "  df_docx.to_csv(f\"output_{lang_code}_{docx_path}\", index=False, encoding=\"utf-8\")\n",
        "\n",
        "  # Display the DataFrame\n",
        "  return df_docx\n",
        "\n",
        "\n",
        "#language_code = \"es\"\n",
        "#UNEP_URL_DOWNREPLACE = \"https://www.unep.org/news-and-stories/press-release/global-annual-finance-flows-7-trillion-fueling-climate-biodiversity#\"\n",
        "\n",
        "def extract_content_by_language(soup):\n",
        "\n",
        "    # Find the div with id=\"field_body\"\n",
        "    field_body_div = soup.find('div', id='field_body')\n",
        "\n",
        "    if field_body_div:\n",
        "        # Helper function to recursively clean div tags deeper than direct children\n",
        "        def clean_div_tags(tag):\n",
        "            for child in tag.children:\n",
        "                if child.name == 'div':\n",
        "                    clean_div_tags(child)\n",
        "                else:\n",
        "                    content.append(str(child))\n",
        "\n",
        "        # Ignore secondary div tags and extract their children tags (except div tags)\n",
        "        content = []\n",
        "        for tag in field_body_div.find_all(recursive=False):\n",
        "            if tag.name == 'div':\n",
        "                # Clean div tags deeper than direct children\n",
        "                clean_div_tags(tag)\n",
        "            else:\n",
        "                # Include children tags (except div tags)\n",
        "                content.append(str(tag))\n",
        "\n",
        "        return ''.join(content).strip()\n",
        "    else:\n",
        "        print(f\"Div with id='field_body' not found in the HTML.\")\n",
        "        return None\n",
        "\n",
        "# Filter video frames and images HTML tags\n",
        "\n",
        "def transform_html_content(html_content):\n",
        "    # Parse the HTML content\n",
        "    soup = BeautifulSoup(html_content, 'html.parser')\n",
        "\n",
        "    # Transform iframe tags with \"youtu\" in src attribute to oembed tags\n",
        "    for iframe_tag in soup.find_all('iframe', src=lambda x: x and 'youtu' in x):\n",
        "        src_attribute = iframe_tag['src']\n",
        "        video_id = src_attribute.split('/')[-1]  # Extract video ID from the src attribute\n",
        "        oembed_tag = soup.new_tag('oembed')\n",
        "        oembed_tag.string = f'https://www.youtube.com/watch?v={video_id}'\n",
        "        iframe_tag.replace_with(oembed_tag)\n",
        "\n",
        "    # Merge figure tags and their children into a single img tag\n",
        "    for figure_tag in soup.find_all('figure'):\n",
        "        img_tag = figure_tag.find('img')\n",
        "        if img_tag:\n",
        "            # Create a new img tag with merged attributes\n",
        "            new_img_tag = soup.new_tag('img')\n",
        "            new_img_tag.attrs = img_tag.attrs\n",
        "            figcaption_tag = figure_tag.find('figcaption')\n",
        "            if figcaption_tag:\n",
        "                # Extract the content of figcaption tag for data-caption attribute\n",
        "                new_img_tag['data-caption'] = str(figcaption_tag.contents[0])\n",
        "            figure_tag.replace_with(new_img_tag)\n",
        "\n",
        "    # Return the modified HTML content\n",
        "    return soup\n",
        "\n",
        "# Link Replacer for HTML\n",
        "\n",
        "def localize_UNEP_html(language_code, soup):\n",
        "    \"\"\"\n",
        "    Localizes the href attributes of <a> tags in HTML content based on the given language code.\n",
        "\n",
        "    Args:\n",
        "        language_code (str): The language code used for URL localization.\n",
        "        soup (BeautifulSoup): The BeautifulSoup object containing the parsed HTML content.\n",
        "\n",
        "    Returns:\n",
        "        str: The modified HTML content with localized href attributes.\n",
        "\n",
        "    Example:\n",
        "        language_code = \"en\"\n",
        "        soup = BeautifulSoup(html_content, 'html.parser')\n",
        "        modified_html = localize_UNEP_html(language_code, soup)\n",
        "        print(modified_html)\n",
        "    \"\"\"\n",
        "    # Access the URL\n",
        "    print(f\"Accessing the URL, type: {type(soup)}\")\n",
        "    soup = get_HTML_generic(soup)\n",
        "    print(f\"Accessing parsed HTML: {type(soup)}\")\n",
        "    # Filter only translatable content\n",
        "    soup = extract_content_by_language(soup)\n",
        "    print(f\"Filtered HTML: {type(soup)}\")\n",
        "\n",
        "    # Transform images and embedded YouTube videos\n",
        "    soup = transform_html_content(soup)\n",
        "    print(f\"Transformed IMG and IFRAME tags: {type(soup)}\")\n",
        "\n",
        "    # Find all <a> tags in the HTML content\n",
        "    for a_tag in soup.find_all('a'):\n",
        "        # Get the current href attribute value\n",
        "        current_href = a_tag.get('href', '')\n",
        "\n",
        "        # Localize the URL using the provided language code\n",
        "        localized_url = localize_URL(current_href, language_code)\n",
        "\n",
        "        # Update the href attribute with the localized URL\n",
        "        if localized_url is not None:\n",
        "          a_tag['href'] = localized_url\n",
        "\n",
        "    # Return the modified HTML content\n",
        "    return str(soup)\n",
        "\n",
        "#Code created by Nelson JAIMES-QUINTERO\n",
        "# --------------------  ## -------------------- ## -------------------- #\n",
        "    #   FUNCTIONS FOR LAUNCHING THE DOCUMENT/LINK PROCESSING    #\n",
        "\n",
        "# DOC-HTML\n",
        "def docx2_bitable(docx_path: str, output_lang: str):\n",
        "    \"\"\"Takes an input doc/docx file and creates a CSV file with 3 columns:\n",
        "    List number, URL found in the file, Localized URL in the input language.\n",
        "    \"\"\"\n",
        "\n",
        "    if not docx_path.lower().endswith(\".doc\") and not docx_path.endswith(\".docx\"):\n",
        "      print(\"ERROR: File not found or is not .DOC nor .DOCX. Verify the input_path field above.\")\n",
        "      return \"ERROR: File not found or is not .DOC nor .DOCX. Verify the input_path field above.\"\n",
        "    input_docx_path = docx_path  # document\n",
        "\n",
        "      # Name the output_file based on the docx's name\n",
        "    last_slash_index = input_docx_path.rfind('/')\n",
        "    if last_slash_index != -1:\n",
        "        extracted_string = f\"{input_docx_path[last_slash_index + 1:]}\"\n",
        "        extracted_string = extracted_string.replace(\"#\", \"\")\n",
        "        #print(extracted_string)\n",
        "    else:\n",
        "        #print(\"No '/' found in the URL.\")\n",
        "        extracted_string = input_docx_path\n",
        "        extracted_string = extracted_string.replace(\"#\", \"\")\n",
        "\n",
        "\n",
        "\n",
        "    # Naming the output file\n",
        "    output_directory = '/content'\n",
        "    output_csv_path = f\"{output_directory}/output_{output_lang}_{extracted_string[0:len(extracted_string)//2]}.csv\"\n",
        "\n",
        "    # Create the output directory if it doesn't exist\n",
        "    os.makedirs(output_directory, exist_ok=True)\n",
        "\n",
        "    #output_csv_path = f\"output_{output_lang}_{docx_path[0:len(docx_path)//2]}.html\"\n",
        "\n",
        "    # Convert DOCX to HTML\n",
        "    html_content = convert_docx_to_html(input_docx_path)\n",
        "    print(\"Doc converted into html successfully.\")\n",
        "    # Write HTML content to a file\n",
        "    #with open(output_html_path, \"w\", encoding=\"utf-8\") as html_file:\n",
        "        #html_file.write(html_content)\n",
        "\n",
        "    #print(\"Conversion complete. HTML file saved at:\", output_html_path)\n",
        "\n",
        "    # Extract href attributes\n",
        "    href_attributes = extract_href_attributes(html_content)\n",
        "    #print(\"Extracted href attributes:\", href_attributes)\n",
        "\n",
        "    output_urls = [localize_URL(url, output_lang) for url in href_attributes]\n",
        "\n",
        "    # Create a pandas DataFrame\n",
        "    df = pd.DataFrame({'index': range(1, len(href_attributes) + 1), 'input_url': href_attributes, 'output_url': output_urls})\n",
        "\n",
        "    # Export the DataFrame to a CSV file\n",
        "    if not df.empty:\n",
        "      print(\"Check your exported csv file at the left on the Folder icon\\nwith the name\", output_csv_path)\n",
        "      df.to_csv(output_csv_path, index=False, encoding=\"utf-8\")\n",
        "\n",
        "    # Display the DataFrame\n",
        "    return df\n",
        "\n",
        "\n",
        "# From PDF file -------------------- ##\n",
        "# NEEDS FITZ\n",
        "def pdf2_bitable(pdf_path: str, output_lang: str):\n",
        "  if not pdf_path.lower().endswith(\"pdf\"):\n",
        "    print(f\"ERROR: File not found or is not .pdf. Verify the input_path field above: {pdf_path}\")\n",
        "    return None\n",
        "  # Create a document object\n",
        "  doc = fitz.open(pdf_path)  # or fitz.Document(filename)\n",
        "\n",
        "  # Create a pandas DataFrame\n",
        "  data = []\n",
        "\n",
        "  # get the links on all pages\n",
        "  for i in range(doc.page_count):\n",
        "      page = doc.load_page(i)\n",
        "      links = page.get_links()\n",
        "      if links:\n",
        "          for item in links:\n",
        "              input_url = item.get('uri')\n",
        "              if input_url != None:\n",
        "                localized_url = localize_URL(input_url, output_lang)\n",
        "                data.append({'index': len(data) + 1, 'Page': i, 'input_url': input_url, 'localized_url': localized_url})\n",
        "\n",
        "\n",
        "\n",
        "  # Create a pandas DataFrame\n",
        "  df_pdf = pd.DataFrame(data)\n",
        "\n",
        "  # Name the file based on the pdf's name\n",
        "  last_slash_index = pdf_path.rfind('/')\n",
        "  if last_slash_index != -1:\n",
        "      extracted_string = f\"{pdf_path[last_slash_index + 1:]}\"\n",
        "      extracted_string = extracted_string.replace(\"#\", \"\")\n",
        "      #print(extracted_string)\n",
        "  else:\n",
        "      #print(\"No '/' found in the URL.\")\n",
        "      extracted_string = pdf_path\n",
        "      extracted_string = extracted_string.replace(\"#\", \"\")\n",
        "\n",
        "\n",
        "\n",
        "  # Naming the output file\n",
        "  output_directory = '/content'\n",
        "  output_csv_path = f\"{output_directory}/output_{output_lang}_{extracted_string[0:len(extracted_string)//2]}.csv\"\n",
        "\n",
        "  # Create the output directory if it doesn't exist\n",
        "  os.makedirs(output_directory, exist_ok=True)\n",
        "\n",
        "  if not df_pdf.empty:\n",
        "    # Export the DataFrame to a CSV file\n",
        "    df_pdf.to_csv(output_csv_path, index=False, encoding=\"utf-8\")\n",
        "    print(\"Check your exported csv file at the left on the Folder icon\\nwith the name\", output_csv_path)\n",
        "    return df_pdf\n",
        "  else:\n",
        "    print(\"ERROR: File not found or is not .pdf. Verify the input_path field above.\")\n",
        "    return None\n",
        "\n",
        "\n",
        "# DOCX REPLACER -------------------- ##\n",
        "\n",
        "#Replace links in Docx with SpireDoc\n",
        "\n",
        "def docx2docx_replacer(my_chemin_docx: str, my_langue):\n",
        "# Create a Document object\n",
        "  doc = Document()\n",
        "\n",
        "  # Load a Word file\n",
        "  doc.LoadFromFile(my_chemin_docx)\n",
        "\n",
        "  # Find all hyperlinks in the document\n",
        "  hyperlinks = []\n",
        "  for i in range(doc.Sections.Count):\n",
        "      section = doc.Sections.get_Item(i)\n",
        "      for j in range(section.Body.ChildObjects.Count):\n",
        "          sec = section.Body.ChildObjects.get_Item(j)\n",
        "          if sec.DocumentObjectType == DocumentObjectType.Paragraph:\n",
        "              for k in range((sec if isinstance(sec, Paragraph) else None).ChildObjects.Count):\n",
        "                  para = (sec if isinstance(sec, Paragraph)\n",
        "                          else None).ChildObjects.get_Item(k)\n",
        "                  if para.DocumentObjectType == DocumentObjectType.Field:\n",
        "                      field = para if isinstance(para, Field) else None\n",
        "                      if field.Type == FieldType.FieldHyperlink:\n",
        "                          hyperlinks.append(field)\n",
        "\n",
        "  # Iterate through hyperlinks and update them\n",
        "  for hyperlink in hyperlinks:\n",
        "      # Get the current display text and URL\n",
        "      current_url = hyperlink.Code.replace('HYPERLINK \"', '').replace('\"', '')\n",
        "      match = re.search(r'HYPERLINK \"(.*?)\"', hyperlink.Code)\n",
        "      if match:\n",
        "          current_url = match.group(1)\n",
        "\n",
        "      current_display_text = hyperlink.FieldText\n",
        "      localized_url = localize_URL(current_url, my_langue)\n",
        "      if localized_url:\n",
        "\n",
        "      # Update the display text and URL of the hyperlink\n",
        "      #hyperlink.FieldText = \"NEW DISPLAY TEXT\"  # Replace with your new display text\n",
        "        hyperlink.Code = f'HYPERLINK \"{localized_url}\"'\n",
        "\n",
        "  if len(hyperlinks)>0:\n",
        "    # Naming output file\n",
        "    last_slash_index = my_chemin_docx.rfind('/')\n",
        "    if last_slash_index != -1:\n",
        "        extracted_string = f\"{my_chemin_docx[last_slash_index + 1:]}\"\n",
        "        extracted_string = extracted_string.replace(\"#\", \"\")\n",
        "        #print(extracted_string)\n",
        "    else:\n",
        "        #print(\"No '/' found in the URL.\")\n",
        "        extracted_string = my_chemin_docx\n",
        "        extracted_string = extracted_string.replace(\"#\", \"\")\n",
        "\n",
        "    output_directory = '/content'\n",
        "    output_path = f\"{output_directory}/output_{my_langue}_{extracted_string[0:len(extracted_string)//2]}.docx\"\n",
        "\n",
        "    # Create the output directory if it doesn't exist\n",
        "    os.makedirs(output_directory, exist_ok=True)\n",
        "\n",
        "\n",
        "\n",
        "    # Save the document to a docx file\n",
        "    print(\"\\n\\nSaving the output file:\")\n",
        "    doc.SaveToFile(output_path, FileFormat.Docx)\n",
        "    print(f\"Output file saved successfuly in your content folder as:\\n\\t{output_path}\")\n",
        "    doc.Close()\n",
        "  else:\n",
        "    print(f\"ERROR on processing the file: {my_chemin_docx}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# 6. HTML downloader and link replacer -------------------- ##\n",
        "\n",
        "def link2_html_converter(UNEP_URL_DOWNREPLACE: str, language_code: str):\n",
        "  \"\"\"Takes an input link from UNEP website. It downloads the webpage\n",
        "  translatable content, replace its links with the localized version and\n",
        "  exports a .txt file with the HTML tags ready to be used in any CAT tool\n",
        "  for human translation.\n",
        "  \"\"\"\n",
        "  modified_html = localize_UNEP_html(language_code, UNEP_URL_DOWNREPLACE)\n",
        "\n",
        "  if not modified_html:\n",
        "    print(\"ERROR: The input URL might not be accessible, or not an URL.\")\n",
        "    raise ValueError(\"The input URL might not be accessible, or not an URL.\")\n",
        "\n",
        "  print(f\"\\nFile to be exported in your folder, or\\n\\n\\t\\tcopy the result from below :\\n\\n\\n{modified_html}\")\n",
        "\n",
        "  # Name the file based on the webpage's name\n",
        "  last_slash_index = UNEP_URL_DOWNREPLACE.rfind('/')\n",
        "  if last_slash_index != -1:\n",
        "      extracted_string = f\"{UNEP_URL_DOWNREPLACE[last_slash_index + 1:]}_replacedURLs_{language_code}.txt\"\n",
        "      extracted_string = extracted_string.replace(\"#\", \"\")\n",
        "      #print(extracted_string)\n",
        "  else:\n",
        "      #print(\"No '/' found in the URL.\")\n",
        "      extracted_string = UNEP_URL_DOWNREPLACE + \".txt\"\n",
        "      extracted_string = extracted_string.replace(\"#\", \"\")\n",
        "\n",
        "  # Save the modified HTML content to a .txt file in the current folder\n",
        "  with open(extracted_string, 'w', encoding='utf-8') as file:\n",
        "      print(type(modified_html))\n",
        "      print(modified_html)\n",
        "      file.write(modified_html)\n",
        "      print(f\"File {extracted_string} exported succesfully\")\n",
        "\n",
        "  # Force download in Google Colab\n",
        "  try:\n",
        "      from google.colab import files\n",
        "      files.download(extracted_string)\n",
        "  except ImportError:\n",
        "      pass\n"
      ],
      "metadata": {
        "id": "7T3vy1uiT-Pl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69400c78-7ad8-4c35-f67d-4336c79e4596"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.5)\n",
            "Collecting pypandoc\n",
            "  Downloading pypandoc-1.13-py3-none-any.whl (21 kB)\n",
            "Installing collected packages: pypandoc\n",
            "Successfully installed pypandoc-1.13\n",
            "Collecting python-docx\n",
            "  Downloading python_docx-1.1.0-py3-none-any.whl (239 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m239.6/239.6 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from python-docx) (4.9.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from python-docx) (4.11.0)\n",
            "Installing collected packages: python-docx\n",
            "Successfully installed python-docx-1.1.0\n",
            "Collecting Spire.Doc\n",
            "  Downloading Spire.Doc-12.3.2-py3-none-manylinux1_x86_64.whl (42.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.7/42.7 MB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting plum-dispatch==1.7.4 (from Spire.Doc)\n",
            "  Downloading plum_dispatch-1.7.4-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: plum-dispatch, Spire.Doc\n",
            "Successfully installed Spire.Doc-12.3.2 plum-dispatch-1.7.4\n",
            "Collecting pymupdf\n",
            "  Downloading PyMuPDF-1.24.2-cp310-none-manylinux2014_x86_64.whl (3.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting PyMuPDFb==1.24.1 (from pymupdf)\n",
            "  Downloading PyMuPDFb-1.24.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (30.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.8/30.8 MB\u001b[0m \u001b[31m30.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyMuPDFb, pymupdf\n",
            "Successfully installed PyMuPDFb-1.24.1 pymupdf-1.24.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 📎 Link extractor for Word docx, PDF or Webpage\n"
      ],
      "metadata": {
        "id": "KOAJprbXjIo9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title *Configure* { display-mode: \"form\", run: \"auto\" }\n",
        "#@markdown # **Define files name and languages**\n",
        "#@markdown ---\n",
        "#@markdown ## 📄 Input text if your file is a Word document or PDF\n",
        "#@markdown Set where your files are located (at the left side menu 📁, mouse-hover the file, click on the 3 dots, **Copy path**):\n",
        "import os\n",
        "from IPython.display import display\n",
        "import ipywidgets as widgets\n",
        "input_path = \"/content/Your March briefing.docx\" #@param {type:\"string\"}\n",
        "#@markdown ---\n",
        "#@markdown ## 🌐 Input text if your text is on a website link\n",
        "#@markdown Paste the website hyperlink here:\n",
        "input_url = \"https://www.unep.org/news-and-stories/story/five-ways-you-can-help-save-environment-mother-earth-day\" #@param {type:\"string\"}\n",
        "#@markdown ---\n",
        "#@markdown ## 🔈What is your target language?\n",
        "#@markdown Select the language of the desired translated URLs:\n",
        "language_code = \"es\" #@param ['es', 'fr', 'sw', 'en', 'zh-hans', 'pt-br', 'ru', 'ar']\n",
        "\n"
      ],
      "metadata": {
        "id": "S56YgwK7jOeV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title *Run* { display-mode: \"form\" }\n",
        "#@markdown # **Let's find the links 🔎☘️**\n",
        "#@markdown Please select the mode ⚠️ and then click on the play button at your left\n",
        "#@markdown - PDF link extractor\n",
        "#@markdown - Docx link extractor\n",
        "#@markdown - Docx link replacer\n",
        "#@markdown - Download from link and replace\n",
        "\n",
        "#@markdown ---\n",
        "finder_mode = \"Download from link and replace\" # @param [\"PDF link extractor\", \"Docx link extractor\", \"Docx link replacer\", \"Download from link and replace\"]\n",
        "%cd /content\n",
        "\n",
        "def run_UNEP_linker(finder_mode:str, in_path: str = \"\", in_url: str = \"\", output_lang: str = \"\"):\n",
        "\n",
        "  print(\"Starting UNEP link hunter...\")\n",
        "  print(\"Selected mode:\", finder_mode)\n",
        "\n",
        "  if finder_mode == \"PDF link extractor\":\n",
        "    output_df = pdf2_bitable(in_path, output_lang)\n",
        "    return output_df\n",
        "\n",
        "  elif finder_mode == \"Docx link extractor\":\n",
        "    #output_df = generate_table_URLs_from_Docx(in_path, output_lang)\n",
        "    print(in_path)\n",
        "    output_df = docx2_bitable(in_path, output_lang) #alternative\n",
        "    return output_df\n",
        "\n",
        "  elif finder_mode == \"Docx link replacer\":\n",
        "\n",
        "\n",
        "    output_df = docx2docx_replacer(in_path, output_lang)\n",
        "    return output_df\n",
        "\n",
        "  elif finder_mode == \"Download from link and replace\":\n",
        "    link2_html_converter(in_url, output_lang)\n",
        "    return None\n",
        "\n",
        "  else:\n",
        "    print(\"Error while loading\")\n",
        "    return\n",
        "\n",
        "  #run main function\n",
        "run_UNEP_linker(finder_mode, input_path, input_url, language_code)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "IktPFAjiXCp5",
        "outputId": "8539a380-9dc3-4564-e9ef-d4bcb7828a60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Starting UNEP link hunter...\n",
            "Selected mode: Download from link and replace\n",
            "Accessing the URL, type: <class 'str'>\n",
            "Scraping successful\n",
            "Accessing parsed HTML: <class 'bs4.BeautifulSoup'>\n",
            "Filtered HTML: <class 'str'>\n",
            "Transformed IMG and IFRAME tags: <class 'bs4.BeautifulSoup'>\n",
            "Validating if https://www.unep.org/news-and-stories/speech/call-new-records-climate-action is an email:\n",
            "Trying find_lang_UNdoc for  https://www.unep.org/news-and-stories/speech/call-new-records-climate-action\n",
            "Scraping successful\n",
            "Trying convert_UNEP_url\n",
            "Scraping successful\n",
            "Validating if https://www.ipbes.net/node/35234 is an email:\n",
            "Trying find_lang_UNdoc for  https://www.ipbes.net/node/35234\n",
            "Scraping successful\n",
            "Trying convert_URL_anyWebsite\n",
            "Scraping successful\n",
            "<class 'bs4.BeautifulSoup'>\n",
            "trying language_link\n",
            "None\n",
            "Validating if https://www.un.org/en/observances/earth-day is an email:\n",
            "Trying find_lang_UNdoc for  https://www.un.org/en/observances/earth-day\n",
            "Scraping successful\n",
            "Trying convert_Intl_Day\n",
            "Validating if https://www.unep.org/explore-topics/climate-action is an email:\n",
            "Trying find_lang_UNdoc for  https://www.unep.org/explore-topics/climate-action\n",
            "Scraping successful\n",
            "Trying convert_UNEP_url\n",
            "Scraping successful\n",
            "Validating if https://www.unep.org/explore-topics/ecosystems-and-biodiversity is an email:\n",
            "Trying find_lang_UNdoc for  https://www.unep.org/explore-topics/ecosystems-and-biodiversity\n",
            "Scraping successful\n",
            "Trying convert_UNEP_url\n",
            "Scraping successful\n",
            "Validating if https://www.unep.org/explore-topics/chemicals-waste is an email:\n",
            "Trying find_lang_UNdoc for  https://www.unep.org/explore-topics/chemicals-waste\n",
            "Scraping successful\n",
            "Trying convert_UNEP_url\n",
            "Scraping successful\n",
            "Validating if https://www.unccd.int/news-stories/stories/latest-climate-report-underscores-urgent-need-act-drought#:~:text=The%20Drought%20in%20Numbers%202022,the%20world's%20population%20by%202050. is an email:\n",
            "Trying find_lang_UNdoc for  https://www.unccd.int/news-stories/stories/latest-climate-report-underscores-urgent-need-act-drought#:~:text=The%20Drought%20in%20Numbers%202022,the%20world's%20population%20by%202050.\n",
            "Scraping successful\n",
            "Trying convert_URL_anyWebsite\n",
            "Scraping successful\n",
            "<class 'bs4.BeautifulSoup'>\n",
            "trying language_link\n",
            "None\n",
            "Validating if https://www.worldenvironmentday.global/ is an email:\n",
            "Trying find_lang_UNdoc for  https://www.worldenvironmentday.global/\n",
            "Scraping successful\n",
            "Trying convert_URL_anyWebsite\n",
            "Scraping successful\n",
            "<class 'bs4.BeautifulSoup'>\n",
            "Scraping successful\n",
            "https://www.worldenvironmentday.global/es/node\n",
            "Validating if https://www.decadeonrestoration.org/publications/ecosystem-restoration-playbook-practical-guide-healing-planet is an email:\n",
            "Trying find_lang_UNdoc for  https://www.decadeonrestoration.org/publications/ecosystem-restoration-playbook-practical-guide-healing-planet\n",
            "Scraping successful\n",
            "Trying convert_URL_anyWebsite\n",
            "Scraping successful\n",
            "<class 'bs4.BeautifulSoup'>\n",
            "Scraping successful\n",
            "https://www.decadeonrestoration.org/publications/ecosystem-restoration-playbook-practical-guide-healing-planet\n",
            "Validating if https://www.decadeonrestoration.org/ is an email:\n",
            "Trying find_lang_UNdoc for  https://www.decadeonrestoration.org/\n",
            "Scraping successful\n",
            "Trying convert_URL_anyWebsite\n",
            "Scraping successful\n",
            "<class 'bs4.BeautifulSoup'>\n",
            "Scraping successful\n",
            "https://www.decadeonrestoration.org/es/node\n",
            "Validating if https://www.unep.org/interactives/things-you-can-do-climate-emergency/ is an email:\n",
            "Trying find_lang_UNdoc for  https://www.unep.org/interactives/things-you-can-do-climate-emergency/\n",
            "Scraping successful\n",
            "Trying convert_UNEP_url\n",
            "Scraping successful\n",
            "Validating if https://wedocs.unep.org/bitstream/handle/20.500.11822/42413/turning_off_the_tap_ESEN.pdf?sequence=8 is an email:\n",
            "Trying find_lang_UNdoc for  https://wedocs.unep.org/bitstream/handle/20.500.11822/42413/turning_off_the_tap_ESEN.pdf?sequence=8\n",
            "URLError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1007)\n",
            "Trying convert_WeDocs_href\n",
            "Validating if https://wedocs.unep.org/xmlui/bitstream/handle/20.500.11822/39764/END%20PLASTIC%20POLLUTION%20-%20TOWARDS%20AN%20INTERNATIONAL%20LEGALLY%20BINDING%20INSTRUMENT%20-%20English.pdf?sequence=1&isAllowed=y is an email:\n",
            "Trying find_lang_UNdoc for  https://wedocs.unep.org/xmlui/bitstream/handle/20.500.11822/39764/END%20PLASTIC%20POLLUTION%20-%20TOWARDS%20AN%20INTERNATIONAL%20LEGALLY%20BINDING%20INSTRUMENT%20-%20English.pdf?sequence=1&isAllowed=y\n",
            "URLError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1007)\n",
            "Trying convert_WeDocs_href\n",
            "Validating if https://www.unep.org/inc-plastic-pollution/session-4 is an email:\n",
            "Trying find_lang_UNdoc for  https://www.unep.org/inc-plastic-pollution/session-4\n",
            "Scraping successful\n",
            "Trying convert_UNEP_url\n",
            "Scraping successful\n",
            "Validating if https://www.unep.org/beatpollution/beat-plastic-pollution/gameplan-it-is-time-to-beat-plastic-pollution is an email:\n",
            "Trying find_lang_UNdoc for  https://www.unep.org/beatpollution/beat-plastic-pollution/gameplan-it-is-time-to-beat-plastic-pollution\n",
            "Scraping successful\n",
            "Trying convert_UNEP_url\n",
            "Scraping successful\n",
            "Validating if https://www.cleanairblueskies.org/ is an email:\n",
            "Trying find_lang_UNdoc for  https://www.cleanairblueskies.org/\n",
            "Scraping successful\n",
            "Trying convert_URL_anyWebsite\n",
            "Scraping successful\n",
            "<class 'bs4.BeautifulSoup'>\n",
            "Scraping successful\n",
            "https://www.cleanairblueskies.org/es\n",
            "Validating if https://www.unep.org/interactives/clean-air-day-guide/#guide is an email:\n",
            "Trying find_lang_UNdoc for  https://www.unep.org/interactives/clean-air-day-guide/#guide\n",
            "Scraping successful\n",
            "Trying convert_UNEP_url\n",
            "Scraping successful\n",
            "Validating if https://academic.oup.com/bioscience/article/70/11/947/5903754 is an email:\n",
            "Trying find_lang_UNdoc for  https://academic.oup.com/bioscience/article/70/11/947/5903754\n",
            "Scraping successful\n",
            "Trying convert_URL_anyWebsite\n",
            "Scraping successful\n",
            "<class 'bs4.BeautifulSoup'>\n",
            "trying language_link\n",
            "None\n",
            "Validating if https://www.decadeonrestoration.org/Interactive/tree-planting-and-ecosystem-restoration-crash-course is an email:\n",
            "Trying find_lang_UNdoc for  https://www.decadeonrestoration.org/Interactive/tree-planting-and-ecosystem-restoration-crash-course\n",
            "Scraping successful\n",
            "Trying convert_URL_anyWebsite\n",
            "Scraping successful\n",
            "<class 'bs4.BeautifulSoup'>\n",
            "Scraping successful\n",
            "https://www.decadeonrestoration.org/Interactive/tree-planting-and-ecosystem-restoration-crash-course\n",
            "Validating if https://www.un.org/en/observances/earth-day is an email:\n",
            "Trying find_lang_UNdoc for  https://www.un.org/en/observances/earth-day\n",
            "Scraping successful\n",
            "Trying convert_Intl_Day\n",
            "Validating if https://www.decadeonrestoration.org/ is an email:\n",
            "Trying find_lang_UNdoc for  https://www.decadeonrestoration.org/\n",
            "Scraping successful\n",
            "Trying convert_URL_anyWebsite\n",
            "Scraping successful\n",
            "<class 'bs4.BeautifulSoup'>\n",
            "Scraping successful\n",
            "https://www.decadeonrestoration.org/es/node\n",
            "Validating if https://www.decadeonrestoration.org/ is an email:\n",
            "Trying find_lang_UNdoc for  https://www.decadeonrestoration.org/\n",
            "Scraping successful\n",
            "Trying convert_URL_anyWebsite\n",
            "Scraping successful\n",
            "<class 'bs4.BeautifulSoup'>\n",
            "Scraping successful\n",
            "https://www.decadeonrestoration.org/es/node\n",
            "\n",
            "File to be exported in your folder, or\n",
            "\n",
            "\t\tcopy the result from below :\n",
            "\n",
            "\n",
            "<p>It is easy to get discouraged by the state of the planet.  </p>\n",
            "<p>Humanity is <a href=\"https://www.unep.org/es/node/36085\" rel=\"noreferrer\">breaking</a> all the wrong records on global warming. Fragile ecosystems face enormous pressure. More than <a href=\"https://www.ipbes.net/node/35234\" rel=\"noreferrer noopener\" target=\"_blank\">1 million</a> plants, animals and other living things are at risk of being wiped out. Dirty air and chemical pollution threaten our land, ocean and health. </p>\n",
            "<p>But <a href=\"https://www.un.org/es/observances/earth-day\" rel=\"noreferrer noopener\" target=\"_blank\">International Mother Earth Day</a>, on 22 April, is a reminder that there is a lot we can do, as individuals, to tackle the triple planetary crisis of <a href=\"https://www.unep.org/es/topics/accion-climatica\" rel=\"noreferrer\">climate change</a>, <a href=\"https://www.unep.org/es/explora-los-temas/ecosistemas\" rel=\"noreferrer\">nature and biodiversity loss</a>, and <a href=\"https://www.unep.org/explore-topics/chemicals-waste\" rel=\"noreferrer\">pollution and waste</a>. </p>\n",
            "<p>“Every action, however big or small, matters to the planet,” said Bruno Pozzi, Deputy Director of the Ecosystems Division of the United Nations Environment Programme (UNEP). “The climate emergency, loss of nature and deadly pollution are not inevitable. We can reverse Earth’s decline but it needs us to come together and for everyone to play their part.” </p>\n",
            "<p>UNEP has developed toolkits for taking environmental action on a range of issues. Here is a guide to five of them: </p>\n",
            "<p><strong>1. Revive the ecosystems that sustain us  </strong></p>\n",
            "<p><img alt=\"A woman holds a small machete in a field in Madagascar  \" data-caption=\"&lt;em&gt;Photo: UNEP&lt;/em&gt;\" data-entity-type=\"file\" data-entity-uuid=\"ce8c59ec-d43c-4528-a350-b67e812b5db8\" height=\"560\" loading=\"lazy\" src=\"https://cdn.unenvironment.org/s3fs-public/inline-images/GEF_CCA_MDG_CoastalCCA-5117_0.jpg\" width=\"840\"/></p><p>Did you know that globally over 2 billion hectares of land are degraded? Or that the number and duration of <a href=\"https://www.unccd.int/news-stories/stories/latest-climate-report-underscores-urgent-need-act-drought#:~:text=The%20Drought%20in%20Numbers%202022,the%20world's%20population%20by%202050.\" rel=\"noreferrer noopener\" target=\"_blank\">droughts</a> has risen by 29 per cent since 2000? Finding solutions to these global problems is crucial. That is why <a href=\"https://www.worldenvironmentday.global/es/node\" rel=\"noreferrer noopener\" target=\"_blank\">World Environment Day</a> on 5 June is focusing on land restoration, desertification and drought resilience. The <a href=\"https://www.decadeonrestoration.org/publications/ecosystem-restoration-playbook-practical-guide-healing-planet\" rel=\"noreferrer noopener\" target=\"_blank\">Ecosystem Restoration Playbook: A Practical Guide to Healing the Planet</a> describes approaches to restoring eight important types of ecosystems – forests, farmlands, grassland and savannahs, rivers and lakes, oceans and coasts, towns and cities, peatlands, and mountains. By taking these recommended actions, you can become part of a <a href=\"https://www.decadeonrestoration.org/es/node\" rel=\"noreferrer noopener\" target=\"_blank\">#GenerationRestoration</a>!  </p>\n",
            "<p><strong>2. Make some noise about climate change </strong></p>\n",
            "<p><img alt=\"Fishermen at work on Kenya’s lake Turkana   \" data-caption=\"&lt;em&gt;Photo: UNEP/Duncan Moore&lt;/em&gt;\" data-entity-type=\"file\" data-entity-uuid=\"78de9512-5f63-42de-be89-cae43c648245\" height=\"560\" loading=\"lazy\" src=\"https://cdn.unenvironment.org/s3fs-public/inline-images/Men%20with%20boat%2C%20fishermen%2C%20Lake%20Turkana.%20Credit%2C%20Duncan%20Moore.jpg\" width=\"840\"/></p><p>The world is in the grip of a climate emergency, a “code red for humanity,” according to the UN Secretary-General. Unless greenhouse gas emissions fall dramatically, warming could pass 2.9°C this century. UNEP’s <a href=\"https://www.unep.org/interactives/things-you-can-do-climate-emergency/\" rel=\"noreferrer\">Act Now: Speak Up</a> campaign shows how citizens can compel governments and businesses to deliver the kind of systemic change needed to limit planetary warming to 1.5°C above pre-industrial levels. </p>\n",
            "<p><strong>3. Conquer the global mountain of plastic </strong></p>\n",
            "<p><img alt=\"A man cleans plastic from a beach in the Republic of Côte d’Ivoire \" data-caption=\"&lt;em&gt;Photo: UNEP/Ollivier Girard &lt;/em&gt;\" data-entity-type=\"file\" data-entity-uuid=\"6a0fa998-33a4-46be-85d9-ba6ee6d07201\" height=\"533\" loading=\"lazy\" src=\"https://cdn.unenvironment.org/s3fs-public/inline-images/20221026_UNEP_WED%202023%20Photojournalism_Abidjan_ollivier%20girard-050.jpg\" width=\"800\"/></p><p>Plastic is everywhere you look. It is in our clothing, household appliances, children’s toys, food packaging, medical devices…the list goes on. While plastic has many uses, our addiction to single-use plastic is disastrous for the planet. It can take thousands, if not tens of thousands, of years to degrade. Yet we continue to produce and consume <a href=\"https://wedocs.unep.org/bitstream/handle/20.500.11822/42413/turning_off_the_tap_ESSP.pdf?sequence=13&amp;isAllowed=y\" rel=\"noreferrer noopener\" target=\"_blank\">430 million tonnes</a> of plastic a year, two-thirds of it quickly ends up as waste dumped in landfills and polluting lakes, rivers, the soil and the ocean.    </p>\n",
            "<p>Recognizing plastic’s impact on climate change, ecosystems, wildlife and the economy, UN Member States agreed on a <a href=\"'NoneType' object has no attribute 'parent'\" rel=\"noreferrer noopener\" target=\"_blank\">resolution</a> to create a legally binding instrument by 2024 to end plastic pollution. Ahead of <a href=\"https://www.unep.org/es/node/35608\" rel=\"noreferrer\">fourth Intergovernmental Negotiating Committee (INC) meeting</a> on the global agreement, UNEP’s <a href=\"https://www.unep.org/beatpollution/beat-plastic-pollution/gameplan-it-is-time-to-beat-plastic-pollution\" rel=\"noreferrer\">Gameplan: It’s time to beat plastic pollution</a> toolkit sets out what individuals can do to help end this environmental scourge. This includes cutting down on unnecessary plastic, choosing to reuse instead of buying new products, supporting brands that are redesigning-out plastic and trying to minimize single-use plastics, and asking governments to adopt circular economy policies and strengthen waste management systems. </p>\n",
            "<p><strong>4. Banish dirty air from the skies </strong></p>\n",
            "<p><img alt=\"A 70-year-old car is parked on a dirt road in Harar, Ethiopia\" data-caption=\"&lt;em&gt;Photo: UNEP/Duncan Moore&lt;/em&gt;\" data-entity-type=\"file\" data-entity-uuid=\"fd7fae99-d3a1-437b-a79b-59eb1f1d15ab\" height=\"483\" loading=\"lazy\" src=\"https://cdn.unenvironment.org/s3fs-public/inline-images/70%20year%20old%20car%20in%20use%20as%20taxi%20credit%20Duncan%20Moore.jpg\" width=\"800\"/></p><p>More than 99 per cent of the global population breathes unsafe air. Air pollution is the biggest environmental health risk of our time, causing an estimated 7 million premature deaths every year. Exposure to dirty air can also cause heart and lung diseases, lung cancer and strokes among other ailments. Air pollutants also harm our natural environment, reducing the oxygen supply in our oceans, making it harder for plants to grow and contributing to the climate crisis. The <a href=\"https://www.cleanairblueskies.org/es\" rel=\"noreferrer noopener\" target=\"_blank\">International Day of Clean air for blue skies</a> on 7 September aims to spread awareness of the problem, while UNEP’s <a href=\"https://www.unep.org/interactives/clean-air-day-guide/#guide\" rel=\"noreferrer\">interactive and guide</a> details the steps you can take to promote cleaner air. </p>\n",
            "<p><strong>5. Get tree planting right </strong></p>\n",
            "<p><img alt=\"Mangrove seedlings are seen at a nursery in Kenya \" data-caption=\"&lt;em&gt;Photo: UNEP/Stephanie Foote&lt;/em&gt;\" data-entity-type=\"file\" data-entity-uuid=\"ddeb8a3e-8cb1-45e4-abfa-552e3991077e\" height=\"533\" loading=\"lazy\" src=\"https://cdn.unenvironment.org/s3fs-public/inline-images/Mangrove%20seedlings%20at%20nursery%2C%20Mida%20Creek%2C%20Kenya%20credit%20Stephanie%20Foote.jpg\" width=\"800\"/></p><p>Trees are amazing. They capture carbon from the atmosphere, protect and fertilize soils, provide a source of firewood and timber, and shelter many animals, birds and insects. No wonder tree planting, to restore ecosystems and counter climate change, has become so popular. But it is not as simple as it sounds. For example, planting <a href=\"https://academic.oup.com/bioscience/article/70/11/947/5903754\" rel=\"noreferrer noopener\" target=\"_blank\">the wrong trees in the wrong places</a> can harm biodiversity and lead to all sorts of unintended consequences. UNEP’s <a href=\"https://www.decadeonrestoration.org/Interactive/tree-planting-and-ecosystem-restoration-crash-course\" rel=\"noreferrer noopener\" target=\"_blank\">Tree planting and ecosystem restoration: a crash course</a> sets out five basic rules for getting it right. </p>\n",
            "<p> </p>\n",
            "<p><em><a href=\"https://www.un.org/es/observances/earth-day\" rel=\"noreferrer noopener\" target=\"_blank\">International Mother Earth Day</a> is celebrated around the world on 22 April. This is the third Mother Earth Day celebrated within the <a href=\"https://www.decadeonrestoration.org/es/node\" rel=\"noreferrer noopener\" target=\"_blank\">UN Decade on Ecosystem Restoration</a>.   </em></p>\n",
            "<p><strong><em>About the UN Decade on Ecosystem Restoration:   </em></strong></p>\n",
            "<p><em>The UN General Assembly has declared 2021–2030 a <a href=\"https://www.decadeonrestoration.org/es/node\" rel=\"noreferrer noopener\" target=\"_blank\">UN Decade on Ecosystem Restoration</a>. Led by the UN Environment Programme and the Food and Agriculture Organization of the UN, together with the support of partners, it is designed to prevent, halt, and reverse the loss and degradation of ecosystems worldwide. It aims at reviving billions of hectares, covering terrestrial as well as aquatic ecosystems. A global call to action, the UN Decade draws together political support, scientific research, and financial muscle to massively scale up restoration.  </em></p>\n",
            "<p> </p>\n",
            "<class 'str'>\n",
            "<p>It is easy to get discouraged by the state of the planet.  </p>\n",
            "<p>Humanity is <a href=\"https://www.unep.org/es/node/36085\" rel=\"noreferrer\">breaking</a> all the wrong records on global warming. Fragile ecosystems face enormous pressure. More than <a href=\"https://www.ipbes.net/node/35234\" rel=\"noreferrer noopener\" target=\"_blank\">1 million</a> plants, animals and other living things are at risk of being wiped out. Dirty air and chemical pollution threaten our land, ocean and health. </p>\n",
            "<p>But <a href=\"https://www.un.org/es/observances/earth-day\" rel=\"noreferrer noopener\" target=\"_blank\">International Mother Earth Day</a>, on 22 April, is a reminder that there is a lot we can do, as individuals, to tackle the triple planetary crisis of <a href=\"https://www.unep.org/es/topics/accion-climatica\" rel=\"noreferrer\">climate change</a>, <a href=\"https://www.unep.org/es/explora-los-temas/ecosistemas\" rel=\"noreferrer\">nature and biodiversity loss</a>, and <a href=\"https://www.unep.org/explore-topics/chemicals-waste\" rel=\"noreferrer\">pollution and waste</a>. </p>\n",
            "<p>“Every action, however big or small, matters to the planet,” said Bruno Pozzi, Deputy Director of the Ecosystems Division of the United Nations Environment Programme (UNEP). “The climate emergency, loss of nature and deadly pollution are not inevitable. We can reverse Earth’s decline but it needs us to come together and for everyone to play their part.” </p>\n",
            "<p>UNEP has developed toolkits for taking environmental action on a range of issues. Here is a guide to five of them: </p>\n",
            "<p><strong>1. Revive the ecosystems that sustain us  </strong></p>\n",
            "<p><img alt=\"A woman holds a small machete in a field in Madagascar  \" data-caption=\"&lt;em&gt;Photo: UNEP&lt;/em&gt;\" data-entity-type=\"file\" data-entity-uuid=\"ce8c59ec-d43c-4528-a350-b67e812b5db8\" height=\"560\" loading=\"lazy\" src=\"https://cdn.unenvironment.org/s3fs-public/inline-images/GEF_CCA_MDG_CoastalCCA-5117_0.jpg\" width=\"840\"/></p><p>Did you know that globally over 2 billion hectares of land are degraded? Or that the number and duration of <a href=\"https://www.unccd.int/news-stories/stories/latest-climate-report-underscores-urgent-need-act-drought#:~:text=The%20Drought%20in%20Numbers%202022,the%20world's%20population%20by%202050.\" rel=\"noreferrer noopener\" target=\"_blank\">droughts</a> has risen by 29 per cent since 2000? Finding solutions to these global problems is crucial. That is why <a href=\"https://www.worldenvironmentday.global/es/node\" rel=\"noreferrer noopener\" target=\"_blank\">World Environment Day</a> on 5 June is focusing on land restoration, desertification and drought resilience. The <a href=\"https://www.decadeonrestoration.org/publications/ecosystem-restoration-playbook-practical-guide-healing-planet\" rel=\"noreferrer noopener\" target=\"_blank\">Ecosystem Restoration Playbook: A Practical Guide to Healing the Planet</a> describes approaches to restoring eight important types of ecosystems – forests, farmlands, grassland and savannahs, rivers and lakes, oceans and coasts, towns and cities, peatlands, and mountains. By taking these recommended actions, you can become part of a <a href=\"https://www.decadeonrestoration.org/es/node\" rel=\"noreferrer noopener\" target=\"_blank\">#GenerationRestoration</a>!  </p>\n",
            "<p><strong>2. Make some noise about climate change </strong></p>\n",
            "<p><img alt=\"Fishermen at work on Kenya’s lake Turkana   \" data-caption=\"&lt;em&gt;Photo: UNEP/Duncan Moore&lt;/em&gt;\" data-entity-type=\"file\" data-entity-uuid=\"78de9512-5f63-42de-be89-cae43c648245\" height=\"560\" loading=\"lazy\" src=\"https://cdn.unenvironment.org/s3fs-public/inline-images/Men%20with%20boat%2C%20fishermen%2C%20Lake%20Turkana.%20Credit%2C%20Duncan%20Moore.jpg\" width=\"840\"/></p><p>The world is in the grip of a climate emergency, a “code red for humanity,” according to the UN Secretary-General. Unless greenhouse gas emissions fall dramatically, warming could pass 2.9°C this century. UNEP’s <a href=\"https://www.unep.org/interactives/things-you-can-do-climate-emergency/\" rel=\"noreferrer\">Act Now: Speak Up</a> campaign shows how citizens can compel governments and businesses to deliver the kind of systemic change needed to limit planetary warming to 1.5°C above pre-industrial levels. </p>\n",
            "<p><strong>3. Conquer the global mountain of plastic </strong></p>\n",
            "<p><img alt=\"A man cleans plastic from a beach in the Republic of Côte d’Ivoire \" data-caption=\"&lt;em&gt;Photo: UNEP/Ollivier Girard &lt;/em&gt;\" data-entity-type=\"file\" data-entity-uuid=\"6a0fa998-33a4-46be-85d9-ba6ee6d07201\" height=\"533\" loading=\"lazy\" src=\"https://cdn.unenvironment.org/s3fs-public/inline-images/20221026_UNEP_WED%202023%20Photojournalism_Abidjan_ollivier%20girard-050.jpg\" width=\"800\"/></p><p>Plastic is everywhere you look. It is in our clothing, household appliances, children’s toys, food packaging, medical devices…the list goes on. While plastic has many uses, our addiction to single-use plastic is disastrous for the planet. It can take thousands, if not tens of thousands, of years to degrade. Yet we continue to produce and consume <a href=\"https://wedocs.unep.org/bitstream/handle/20.500.11822/42413/turning_off_the_tap_ESSP.pdf?sequence=13&amp;isAllowed=y\" rel=\"noreferrer noopener\" target=\"_blank\">430 million tonnes</a> of plastic a year, two-thirds of it quickly ends up as waste dumped in landfills and polluting lakes, rivers, the soil and the ocean.    </p>\n",
            "<p>Recognizing plastic’s impact on climate change, ecosystems, wildlife and the economy, UN Member States agreed on a <a href=\"'NoneType' object has no attribute 'parent'\" rel=\"noreferrer noopener\" target=\"_blank\">resolution</a> to create a legally binding instrument by 2024 to end plastic pollution. Ahead of <a href=\"https://www.unep.org/es/node/35608\" rel=\"noreferrer\">fourth Intergovernmental Negotiating Committee (INC) meeting</a> on the global agreement, UNEP’s <a href=\"https://www.unep.org/beatpollution/beat-plastic-pollution/gameplan-it-is-time-to-beat-plastic-pollution\" rel=\"noreferrer\">Gameplan: It’s time to beat plastic pollution</a> toolkit sets out what individuals can do to help end this environmental scourge. This includes cutting down on unnecessary plastic, choosing to reuse instead of buying new products, supporting brands that are redesigning-out plastic and trying to minimize single-use plastics, and asking governments to adopt circular economy policies and strengthen waste management systems. </p>\n",
            "<p><strong>4. Banish dirty air from the skies </strong></p>\n",
            "<p><img alt=\"A 70-year-old car is parked on a dirt road in Harar, Ethiopia\" data-caption=\"&lt;em&gt;Photo: UNEP/Duncan Moore&lt;/em&gt;\" data-entity-type=\"file\" data-entity-uuid=\"fd7fae99-d3a1-437b-a79b-59eb1f1d15ab\" height=\"483\" loading=\"lazy\" src=\"https://cdn.unenvironment.org/s3fs-public/inline-images/70%20year%20old%20car%20in%20use%20as%20taxi%20credit%20Duncan%20Moore.jpg\" width=\"800\"/></p><p>More than 99 per cent of the global population breathes unsafe air. Air pollution is the biggest environmental health risk of our time, causing an estimated 7 million premature deaths every year. Exposure to dirty air can also cause heart and lung diseases, lung cancer and strokes among other ailments. Air pollutants also harm our natural environment, reducing the oxygen supply in our oceans, making it harder for plants to grow and contributing to the climate crisis. The <a href=\"https://www.cleanairblueskies.org/es\" rel=\"noreferrer noopener\" target=\"_blank\">International Day of Clean air for blue skies</a> on 7 September aims to spread awareness of the problem, while UNEP’s <a href=\"https://www.unep.org/interactives/clean-air-day-guide/#guide\" rel=\"noreferrer\">interactive and guide</a> details the steps you can take to promote cleaner air. </p>\n",
            "<p><strong>5. Get tree planting right </strong></p>\n",
            "<p><img alt=\"Mangrove seedlings are seen at a nursery in Kenya \" data-caption=\"&lt;em&gt;Photo: UNEP/Stephanie Foote&lt;/em&gt;\" data-entity-type=\"file\" data-entity-uuid=\"ddeb8a3e-8cb1-45e4-abfa-552e3991077e\" height=\"533\" loading=\"lazy\" src=\"https://cdn.unenvironment.org/s3fs-public/inline-images/Mangrove%20seedlings%20at%20nursery%2C%20Mida%20Creek%2C%20Kenya%20credit%20Stephanie%20Foote.jpg\" width=\"800\"/></p><p>Trees are amazing. They capture carbon from the atmosphere, protect and fertilize soils, provide a source of firewood and timber, and shelter many animals, birds and insects. No wonder tree planting, to restore ecosystems and counter climate change, has become so popular. But it is not as simple as it sounds. For example, planting <a href=\"https://academic.oup.com/bioscience/article/70/11/947/5903754\" rel=\"noreferrer noopener\" target=\"_blank\">the wrong trees in the wrong places</a> can harm biodiversity and lead to all sorts of unintended consequences. UNEP’s <a href=\"https://www.decadeonrestoration.org/Interactive/tree-planting-and-ecosystem-restoration-crash-course\" rel=\"noreferrer noopener\" target=\"_blank\">Tree planting and ecosystem restoration: a crash course</a> sets out five basic rules for getting it right. </p>\n",
            "<p> </p>\n",
            "<p><em><a href=\"https://www.un.org/es/observances/earth-day\" rel=\"noreferrer noopener\" target=\"_blank\">International Mother Earth Day</a> is celebrated around the world on 22 April. This is the third Mother Earth Day celebrated within the <a href=\"https://www.decadeonrestoration.org/es/node\" rel=\"noreferrer noopener\" target=\"_blank\">UN Decade on Ecosystem Restoration</a>.   </em></p>\n",
            "<p><strong><em>About the UN Decade on Ecosystem Restoration:   </em></strong></p>\n",
            "<p><em>The UN General Assembly has declared 2021–2030 a <a href=\"https://www.decadeonrestoration.org/es/node\" rel=\"noreferrer noopener\" target=\"_blank\">UN Decade on Ecosystem Restoration</a>. Led by the UN Environment Programme and the Food and Agriculture Organization of the UN, together with the support of partners, it is designed to prevent, halt, and reverse the loss and degradation of ecosystems worldwide. It aims at reviving billions of hectares, covering terrestrial as well as aquatic ecosystems. A global call to action, the UN Decade draws together political support, scientific research, and financial muscle to massively scale up restoration.  </em></p>\n",
            "<p> </p>\n",
            "File five-ways-you-can-help-save-environment-mother-earth-day_replacedURLs_es.txt exported succesfully\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_93b7a6c6-4644-4ca1-b6b4-f6b024077cd1\", \"five-ways-you-can-help-save-environment-mother-earth-day_replacedURLs_es.txt\", 10301)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Playground for tests"
      ],
      "metadata": {
        "id": "NcUTVrzmQJHj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# URL for testing\n",
        "url1 = \"https://www.unep.org/interactive/explore-ecosystems/mountains/en/index.php#/mountain-intro\" #UN Decade\n",
        "url2 = \"https://www.unesco.org/en/decades/ocean-decade\" # UNESCO\n",
        "url3 = \"https://www.unep.org/interactive/flagship-initiatives-boosting-nature-livelihoods/\" # UNEP UNDecade Ecosystems +/es\n",
        "url4 = \"https://www.unesco.org/es\" #landing UNESCO\n",
        "# article test urls\n",
        "url5 = \"https://www.unesco.org/es/articles/diez-jovenes-poetas-del-caribe-presentaron-sus-obras-en-el-mercado-de-la-poesia-de-paris\"\n",
        "url6 = \"https://www.who.int/health-topics/antimicrobial-resistance\" #WHO topic\n",
        "url7 = \"https://www.who.int/es/news/item/27-12-2023-who-teams-deliver-supplies-to-hospitals-in-northern-and-southern-gaza\" #WHO news\n",
        "url8_wmo = \"https://wmo.int/news/media-centre/2023-shatters-climate-records-major-impacts\"\n",
        "url9_unops = \"https://www.unops.org/es/news-and-stories/news/natures-critical-role-in-infrastructure-for-sustainable-development\" #unops news\n",
        "url10_fao = \"https://www.fao.org/one-health/es\" #Fao topic\n",
        "url11_fao = \"https://www.fao.org/newsroom/detail/UN-AMR-FAO-UNEP-WHO-WOAH-Multistakeholder-Platform-Plenary-2023/en\" #FAO news\n",
        "\n",
        "# Replace url6 with the URL between quote marks\n",
        "  # Example: localize_URL(\"https://www.fao.org/one-health/\", \"fr\")\n",
        "result = localize_URL(\"https://www.fao.org/one-health/\", \"fr\")\n",
        "print(result)"
      ],
      "metadata": {
        "id": "CyntJsSqDLYz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3697fe9-5fbe-4eff-b31b-b1084560e60b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validating if https://www.fao.org/one-health/ is an email:\n",
            "Trying find_lang_UNdoc for  https://www.fao.org/one-health/\n",
            "Scraping successful\n",
            "Trying convert_URL_anyWebsite\n",
            "Scraping successful\n",
            "<class 'bs4.BeautifulSoup'>\n",
            "trying WHO\n",
            "[<input data-sf-role=\"fr\" type=\"hidden\" value=\"https://www.fao.org/one-health/home/fr\"/>]\n",
            "https://www.fao.org/one-health/home/fr\n",
            "https://www.fao.org/one-health/home/fr\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Future improvements\n",
        "\n",
        "- Compatibility with UNEP press releases\n",
        "- UN HQ news finder\n",
        "- Compatibility with UNHCR website\n",
        "- Chinese and Portuguese broader compatibility\n",
        "- Time out feature for never-ending loops\n",
        "- Insert [PDF annotations](https://artifex.com/blog/working-with-pdf-annotations-in-python) to highlight the URL to be changed\n",
        "- Replace PDF's links with the localized links\n",
        "- Try a file opener [dialog](https://docs.python.org/3/library/dialog.html)?"
      ],
      "metadata": {
        "id": "x22CiLrpQZ4y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#HTML import and localizer\n",
        "with open(\"/content/motherDay.html\", \"r\", encoding=\"utf-8\") as f:\n",
        "        #html_file.write(html_content)\n",
        "  # Find all <a> tags in the HTML content\n",
        "  soup = BeautifulSoup(f, 'html.parser')\n",
        "  for a_tag in soup.find_all('a'):\n",
        "      # Get the current href attribute value\n",
        "      current_href = a_tag.get('href', '')\n",
        "\n",
        "      # Localize the URL using the provided language code\n",
        "      localized_url = localize_URL(current_href, \"es\")\n",
        "\n",
        "      # Update the href attribute with the localized URL\n",
        "      if localized_url is not None:\n",
        "        a_tag['href'] = localized_url\n",
        "\n",
        "  # Return the modified HTML content\n",
        "  output = str(soup)\n",
        "  print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-D8mrp7ZRIv4",
        "outputId": "8d061ccf-2746-4264-edf8-f9f9ab70cc96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<p>Es fácil sentirse desesperanzado por el estado del planeta. </p>\n",
            "<p>La humanidad está batiendo todos los récords equivocados en materia de calentamiento global. Los ecosistemas frágiles se enfrentan a grandes amenazas. Más de un millón de plantas, animales y otros seres vivos corren el riesgo de desaparecer. El aire sucio y la contaminación química amenazan nuestra tierra, nuestros océanos y nuestra salud.</p>\n",
            "<p>Sin embargo, el Día Internacional de la Madre Tierra, que se celebra el 22 de abril, nos recuerda que podemos hacer mucho, como individuos, para hacer frente a la triple crisis planetaria del cambio climático, la pérdida de naturaleza y biodiversidad, y la contaminación y los desechos.</p>\n",
            "<p>\"Cada acción, por grande o pequeña que sea, es importante para el planeta\", afirmó Bruno Pozzi, Director Adjunto de la División de Ecosistemas del Programa de las Naciones Unidas para el Medio Ambiente (PNUMA). \"La emergencia climática, la pérdida de naturaleza y la contaminación mortal no son inevitables. Podemos revertir el declive de la Tierra, pero hace falta que nos unamos y que cada uno cumpla su parte\".</p>\n",
            "<p>El PNUMA ha elaborado múltiples herramientas para tomar medidas ambientales en diversos ámbitos. A continuación presentamos cinco de ellas:</p>\n",
            "<p> </p>\n",
            "<p><strong>1. Revitalizar los ecosistemas que nos sustentan </strong></p>\n",
            "<img alt=\"A woman holds a small machete in a field in Madagascar  \" data-caption=\"&lt;em&gt;Photo: UNEP&lt;/em&gt;\" data-entity-type=\"file\" data-entity-uuid=\"ce8c59ec-d43c-4528-a350-b67e812b5db8\" src=\"https://cdn.unenvironment.org/s3fs-public/inline-images/GEF_CCA_MDG_CoastalCCA-5117_0.jpg\"/>\n",
            "<p>¿Sabía que en el mundo hay más de 2.000 millones de hectáreas de tierras degradadas? ¿O que el número y la duración de las sequías han aumentado un 29% desde 2000? Encontrar soluciones a estos problemas mundiales es crucial. Por eso, el Día Mundial del Medio Ambiente, que se celebra el 5 de junio, se centra en la restauración de la tierra, la desertificación y la resistencia a la sequía. El Manual de Restauración de Ecosistemas: Una guía práctica para sanar el planeta describe enfoques para restaurar ocho tipos importantes de ecosistemas: bosques, tierras de cultivo, praderas y sabanas, ríos y lagos, océanos y costas, pueblos y ciudades, turberas y montañas. Llevando a cabo estas acciones recomendadas, ¡puedes formar parte de una #GeneraciónRestauración! </p>\n",
            "<p><strong>2. Alza la voz sobre el cambio climático </strong></p>\n",
            "<img alt=\"Fishermen at work on Kenya’s lake Turkana   \" data-caption=\"&lt;em&gt;Photo: UNEP/Duncan Moore&lt;/em&gt;\" data-entity-type=\"file\" data-entity-uuid=\"78de9512-5f63-42de-be89-cae43c648245\" src=\"https://cdn.unenvironment.org/s3fs-public/inline-images/Men%20with%20boat%2C%20fishermen%2C%20Lake%20Turkana.%20Credit%2C%20Duncan%20Moore.jpg\"/>\n",
            "<p>El mundo está sumido en una emergencia climática, un \"código rojo para la humanidad\", según el Secretario General de la ONU. A menos que las emisiones de gases de efecto invernadero disminuyan drásticamente, el calentamiento podría superar los 2,9 °C este siglo. La campaña Actúa Ahora: Alza la Voz del PNUMA muestra cómo la ciudadanía puede presionar a los gobiernos y a las empresas a realizar el tipo de cambio sistémico necesario para limitar el calentamiento planetario a 1,5 °C por encima de los niveles preindustriales.  </p>\n",
            "<p><strong>3. Vencer la montaña mundial de plástico  </strong></p>\n",
            "<img alt=\"A man cleans plastic from a beach in the Republic of Côte d’Ivoire \" data-caption=\"&lt;em&gt;Photo: UNEP/Ollivier Girard&amp;nbsp;&lt;/em&gt;\" data-entity-type=\"file\" data-entity-uuid=\"6a0fa998-33a4-46be-85d9-ba6ee6d07201\" src=\"https://cdn.unenvironment.org/s3fs-public/inline-images/20221026_UNEP_WED%202023%20Photojournalism_Abidjan_ollivier%20girard-050.jpg\"/>\n",
            "<p>El plástico está en todas partes. Está en nuestra ropa, en los electrodomésticos, en los juguetes de los niños, en los envases de los alimentos, en los dispositivos médicos... y la lista continúa. Aunque el plástico tiene muchos usos, nuestra adicción al plástico de un solo uso es desastrosa para el planeta. Puede tardar miles, si no decenas de miles, de años en degradarse. Sin embargo, seguimos produciendo y consumiendo 430 millones de toneladas de plástico al año, dos tercios de las cuales acaban rápidamente como residuos vertidos en vertederos y contaminando lagos, ríos, el suelo y los océanos.   </p>\n",
            "<p>Conscientes del impacto del plástico en el cambio climático, los ecosistemas, la vida silvestre y la economía, los Estados miembros de la ONU acordaron una resolución para crear un instrumento jurídicamente vinculante para 2024 con el fin de poner fin a la contaminación por plástico. Previo al cuarto período de sesiones del Comité Intergubernamental de Negociación (CIN) sobre el acuerdo mundial, el PNUMA ha publicado Gameplan: Es hora de vivir sin contaminación por plásticos, en el que expone lo que las personas pueden hacer para erradicar esta lacra medioambiental. Esto incluye reducir el plástico innecesario, optar por la reutilización en lugar de comprar nuevos productos, apoyar a las marcas que están rediseñando el plástico y tratando de minimizar los plásticos de un solo uso, y pedir a los gobiernos que adopten políticas de economía circular y fortalezcan los sistemas de gestión de residuos.</p>\n",
            "<p> </p>\n",
            "<p><strong>4. Eliminar el aire sucio de los cielos</strong></p>\n",
            "<img alt=\"A 70-year-old car is parked on a dirt road in Harar, Ethiopia\" data-caption=\"&lt;em&gt;Photo: UNEP/Duncan Moore&lt;/em&gt;\" data-entity-type=\"file\" data-entity-uuid=\"fd7fae99-d3a1-437b-a79b-59eb1f1d15ab\" src=\"https://cdn.unenvironment.org/s3fs-public/inline-images/70%20year%20old%20car%20in%20use%20as%20taxi%20credit%20Duncan%20Moore.jpg\"/>\n",
            "<p>Más del 99% de la población mundial respira aire inseguro. La contaminación atmosférica es el mayor riesgo medioambiental para la salud de nuestro tiempo, y se calcula que causa 7 millones de muertes prematuras al año. La exposición al aire sucio también puede causar enfermedades cardíacas y pulmonares, cáncer de pulmón y accidentes cerebrovasculares, entre otras dolencias. Asimismo, los contaminantes atmosféricos dañan nuestro entorno natural, ya que reducen el suministro de oxígeno en nuestros océanos, dificultan el crecimiento de las plantas y agravan la crisis climática. El Día Internacional del Aire Limpio por un cielo azul, que se celebra el 7 de septiembre, pretende concienciar sobre el problema, mientras que la guía e interactiva del PNUMA detalla los pasos que puedes dar para promover un aire más limpio.</p>\n",
            "<p> </p>\n",
            "<p><strong>5. Plantar bien los árboles</strong></p>\n",
            "<img alt=\"Mangrove seedlings are seen at a nursery in Kenya \" data-caption=\"&lt;em&gt;Photo: UNEP/Stephanie Foote&lt;/em&gt;\" data-entity-type=\"file\" data-entity-uuid=\"ddeb8a3e-8cb1-45e4-abfa-552e3991077e\" src=\"https://cdn.unenvironment.org/s3fs-public/inline-images/Mangrove%20seedlings%20at%20nursery%2C%20Mida%20Creek%2C%20Kenya%20credit%20Stephanie%20Foote.jpg\"/>\n",
            "<p>Los árboles son increíbles. Capturan carbono de la atmósfera, protegen y fertilizan los suelos, proporcionan una fuente de leña y madera, y dan cobijo a muchos animales, aves e insectos. No es de extrañar que la plantación de árboles, para restaurar los ecosistemas y contrarrestar el cambio climático, se haya hecho tan popular. Pero no es tan sencillo como parece. Por ejemplo, plantar los árboles equivocados en los lugares equivocados puede dañar la biodiversidad y provocar todo tipo de consecuencias imprevistas. La publicación del PNUMA Plantación de árboles y restauración de ecosistemas: un curso intensivo establece cinco reglas básicas para hacerlo bien.</p>\n",
            "<p> </p>\n",
            "<p> </p>\n",
            "<p><em>El 22 de abril se celebra en todo el mundo el Día Internacional de la Madre Tierra. Este es el tercer Día de la Madre Tierra que se celebra dentro del Decenio de las Naciones Unidas sobre la Restauración de los Ecosistemas.   </em></p>\n",
            "<p> </p>\n",
            "<p><em><strong>Acerca del Decenio de las Naciones Unidas sobre la Restauración de los Ecosistemas:  </strong> </em></p>\n",
            "<p><em>La Asamblea General de las Naciones Unidas proclamó el período 2021-2030 Decenio de las Naciones Unidas sobre la Restauración de los Ecosistemas. Dirigido por el Programa de las Naciones Unidas para el Medio Ambiente (PNUMA) y la Organización de las Naciones Unidas para la Agricultura y la Alimentación (FAO), con el apoyo de sus socios, está diseñado para prevenir, detener y revertir la pérdida y degradación de los ecosistemas en todo el mundo. Su objetivo es revitalizar miles de millones de hectáreas, tanto de ecosistemas terrestres como acuáticos. El Decenio de las Naciones Unidas, un llamamiento mundial a la acción, aúna el apoyo político, la investigación científica y la fuerza financiera para incrementar a gran escala la restauración. </em></p>\n",
            "<p> </p>\n",
            "\n"
          ]
        }
      ]
    }
  ]
}